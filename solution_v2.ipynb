{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ITdyMoBlcTIAe4O9N78nPgGRB42MafoA",
      "authorship_tag": "ABX9TyOw4qQbxIrJAMPkuoSTgQAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alan-Hans/Challenge-Data-Scientist/blob/developement/solution_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge requires to predict the probability of delay of flights at Santiago de Chile Airport (SCL) using a provided dataset. This work was developed by Alan Hans Bitterlich Koning, email: Alan.bitterlich.k@gmail.com. The index of this notebook is the following: \n",
        "```\n",
        "1.- Import data \n",
        "2.- EDA\n",
        "3.- Models\n",
        "4.- Results\n",
        "5.- Comments\n"
      ],
      "metadata": {
        "id": "YfjHgLyNgSb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.- Import data"
      ],
      "metadata": {
        "id": "nbOzO4KNh3W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Alan-Hans/Challenge-Data-Scientist/main/dataset_SCL.csv'\n",
        "\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZj54iCJgF3J",
        "outputId": "a9665404-0e13-4067-ca9e-339c401263ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b0cb3f0119c9>:9: DtypeWarning: Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(url)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "3PvBRIavyPl7",
        "outputId": "6169f8af-0501-4c80-be2e-b06d96d8c884"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
              "0      2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   \n",
              "1      2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   \n",
              "2      2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   \n",
              "3      2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   \n",
              "4      2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   \n",
              "...                    ...   ...   ...   ...   ...                  ...   \n",
              "68201  2017-12-22 14:55:00   400  SCEL  SPJC   JAT  2017-12-22 15:41:00   \n",
              "68202  2017-12-25 14:55:00   400  SCEL  SPJC   JAT  2017-12-25 15:11:00   \n",
              "68203  2017-12-27 14:55:00   400  SCEL  SPJC   JAT  2017-12-27 15:35:00   \n",
              "68204  2017-12-29 14:55:00   400  SCEL  SPJC   JAT  2017-12-29 15:08:00   \n",
              "68205  2017-12-31 14:55:00   400  SCEL  SPJC   JAT  2017-12-31 15:04:00   \n",
              "\n",
              "       Vlo-O Ori-O Des-O Emp-O  DIA  MES   AÑO     DIANOM TIPOVUELO  \\\n",
              "0        226  SCEL  KMIA   AAL    1    1  2017    Domingo         I   \n",
              "1        226  SCEL  KMIA   AAL    2    1  2017      Lunes         I   \n",
              "2        226  SCEL  KMIA   AAL    3    1  2017     Martes         I   \n",
              "3        226  SCEL  KMIA   AAL    4    1  2017  Miercoles         I   \n",
              "4        226  SCEL  KMIA   AAL    5    1  2017     Jueves         I   \n",
              "...      ...   ...   ...   ...  ...  ...   ...        ...       ...   \n",
              "68201  400.0  SCEL  SPJC   JAT   22   12  2017    Viernes         I   \n",
              "68202  400.0  SCEL  SPJC   JAT   25   12  2017      Lunes         I   \n",
              "68203  400.0  SCEL  SPJC   JAT   27   12  2017  Miercoles         I   \n",
              "68204  400.0  SCEL  SPJC   JAT   29   12  2017    Viernes         I   \n",
              "68205  400.0  SCEL  SPJC   JAT   31   12  2017    Domingo         I   \n",
              "\n",
              "                   OPERA  SIGLAORI SIGLADES  \n",
              "0      American Airlines  Santiago    Miami  \n",
              "1      American Airlines  Santiago    Miami  \n",
              "2      American Airlines  Santiago    Miami  \n",
              "3      American Airlines  Santiago    Miami  \n",
              "4      American Airlines  Santiago    Miami  \n",
              "...                  ...       ...      ...  \n",
              "68201       JetSmart SPA  Santiago     Lima  \n",
              "68202       JetSmart SPA  Santiago     Lima  \n",
              "68203       JetSmart SPA  Santiago     Lima  \n",
              "68204       JetSmart SPA  Santiago     Lima  \n",
              "68205       JetSmart SPA  Santiago     Lima  \n",
              "\n",
              "[68206 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3070247-5240-4100-a9cb-f19aad886b36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha-I</th>\n",
              "      <th>Vlo-I</th>\n",
              "      <th>Ori-I</th>\n",
              "      <th>Des-I</th>\n",
              "      <th>Emp-I</th>\n",
              "      <th>Fecha-O</th>\n",
              "      <th>Vlo-O</th>\n",
              "      <th>Ori-O</th>\n",
              "      <th>Des-O</th>\n",
              "      <th>Emp-O</th>\n",
              "      <th>DIA</th>\n",
              "      <th>MES</th>\n",
              "      <th>AÑO</th>\n",
              "      <th>DIANOM</th>\n",
              "      <th>TIPOVUELO</th>\n",
              "      <th>OPERA</th>\n",
              "      <th>SIGLAORI</th>\n",
              "      <th>SIGLADES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Lunes</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Martes</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Miercoles</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Jueves</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68201</th>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Viernes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68202</th>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>25</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Lunes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68203</th>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>27</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Miercoles</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68204</th>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Viernes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68205</th>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68206 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3070247-5240-4100-a9cb-f19aad886b36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3070247-5240-4100-a9cb-f19aad886b36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3070247-5240-4100-a9cb-f19aad886b36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of every column\n",
        "```\n",
        "Fecha-I: Scheduled date and time of departure (local time) in format \"YYYY-MM-DD hh:mm:ss\" (year-month-day hour:minute:second).\n",
        "Vlo-I: Scheduled flight number (text)\n",
        "Ori-I: Origin city code (text)\n",
        "Des-I: destination city code. (text)\n",
        "Emp-I: Scheduled airline code (text)\n",
        "Fecha-O: Date and time of arrival (local time) in format \"YYYY-MM-DD hh:mm:ss\" (year-month-day hour:minute:second).\n",
        "Vlo-O: Flight operation number of the flight (text)\n",
        "Ori-O: Operation origin city code (text)\n",
        "Des-O: Operation destination city code (text)\n",
        "Emp-O: Airline code of the operated flight (text)\n",
        "DIA: Day of the month (numeric)\n",
        "MES: Month of the year (numeric)\n",
        "AÑO: Year (numeric)\n",
        "DIANOM: Day of the week (text)\n",
        "TIPOVUELO: Type of flight, I =International, N =National (text)\n",
        "OPERA: Operating airline company (text)\n",
        "SIGLAORI: Name city of origin (text)\n",
        "SIGLADES: Destination city name (text)'\n",
        "\n"
      ],
      "metadata": {
        "id": "GBBrqwvr5Vuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.- EDA"
      ],
      "metadata": {
        "id": "dDOyioTkyZEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "#Some interesting insights from this analysis: \n",
        "\n",
        "*   It shows that the largest number of flights in this data correspond to Grupo LATAM airlines with about 58%.\n",
        "*   The Top 5 destinations are: Buenos Aires,     Antofagasta,   Lima,             \n",
        "Calama and           Puerto Montt.\n",
        "*   Most flights are made during the month of December and on Fridays.\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "Du4GcvcDDTnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print the first few rows of the dataset to get a sense of its structure\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(df.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Descriptive statistics\n",
        "print(df.describe())\n",
        "\n",
        "# List of columns\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtVdCHLMzTDa",
        "outputId": "f55999c7-51ff-4a7a-cda2-a9fd33503c97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O Vlo-O  \\\n",
            "0  2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   226   \n",
            "1  2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   226   \n",
            "2  2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   226   \n",
            "3  2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   226   \n",
            "4  2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   226   \n",
            "\n",
            "  Ori-O Des-O Emp-O  DIA  MES   AÑO     DIANOM TIPOVUELO              OPERA  \\\n",
            "0  SCEL  KMIA   AAL    1    1  2017    Domingo         I  American Airlines   \n",
            "1  SCEL  KMIA   AAL    2    1  2017      Lunes         I  American Airlines   \n",
            "2  SCEL  KMIA   AAL    3    1  2017     Martes         I  American Airlines   \n",
            "3  SCEL  KMIA   AAL    4    1  2017  Miercoles         I  American Airlines   \n",
            "4  SCEL  KMIA   AAL    5    1  2017     Jueves         I  American Airlines   \n",
            "\n",
            "   SIGLAORI SIGLADES  \n",
            "0  Santiago    Miami  \n",
            "1  Santiago    Miami  \n",
            "2  Santiago    Miami  \n",
            "3  Santiago    Miami  \n",
            "4  Santiago    Miami  \n",
            "(68206, 18)\n",
            "Fecha-I      0\n",
            "Vlo-I        0\n",
            "Ori-I        0\n",
            "Des-I        0\n",
            "Emp-I        0\n",
            "Fecha-O      0\n",
            "Vlo-O        1\n",
            "Ori-O        0\n",
            "Des-O        0\n",
            "Emp-O        0\n",
            "DIA          0\n",
            "MES          0\n",
            "AÑO          0\n",
            "DIANOM       0\n",
            "TIPOVUELO    0\n",
            "OPERA        0\n",
            "SIGLAORI     0\n",
            "SIGLADES     0\n",
            "dtype: int64\n",
            "                DIA           MES           AÑO\n",
            "count  68206.000000  68206.000000  68206.000000\n",
            "mean      15.714790      6.622585   2017.000029\n",
            "std        8.782886      3.523321      0.005415\n",
            "min        1.000000      1.000000   2017.000000\n",
            "25%        8.000000      3.000000   2017.000000\n",
            "50%       16.000000      7.000000   2017.000000\n",
            "75%       23.000000     10.000000   2017.000000\n",
            "max       31.000000     12.000000   2018.000000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 68206 entries, 0 to 68205\n",
            "Data columns (total 18 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Fecha-I    68206 non-null  object\n",
            " 1   Vlo-I      68206 non-null  object\n",
            " 2   Ori-I      68206 non-null  object\n",
            " 3   Des-I      68206 non-null  object\n",
            " 4   Emp-I      68206 non-null  object\n",
            " 5   Fecha-O    68206 non-null  object\n",
            " 6   Vlo-O      68205 non-null  object\n",
            " 7   Ori-O      68206 non-null  object\n",
            " 8   Des-O      68206 non-null  object\n",
            " 9   Emp-O      68206 non-null  object\n",
            " 10  DIA        68206 non-null  int64 \n",
            " 11  MES        68206 non-null  int64 \n",
            " 12  AÑO        68206 non-null  int64 \n",
            " 13  DIANOM     68206 non-null  object\n",
            " 14  TIPOVUELO  68206 non-null  object\n",
            " 15  OPERA      68206 non-null  object\n",
            " 16  SIGLAORI   68206 non-null  object\n",
            " 17  SIGLADES   68206 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 9.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## For the purpose of this analysis, null results will be filtered out, which apparently corresponds to only 1, so it will not have a great impact and drop duplicate if there are.\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xlSBRBj1YwL",
        "outputId": "608e5920-5ce9-47ea-d0de-a00d2ecfc993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 18 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Fecha-I    68205 non-null  object\n",
            " 1   Vlo-I      68205 non-null  object\n",
            " 2   Ori-I      68205 non-null  object\n",
            " 3   Des-I      68205 non-null  object\n",
            " 4   Emp-I      68205 non-null  object\n",
            " 5   Fecha-O    68205 non-null  object\n",
            " 6   Vlo-O      68205 non-null  object\n",
            " 7   Ori-O      68205 non-null  object\n",
            " 8   Des-O      68205 non-null  object\n",
            " 9   Emp-O      68205 non-null  object\n",
            " 10  DIA        68205 non-null  int64 \n",
            " 11  MES        68205 non-null  int64 \n",
            " 12  AÑO        68205 non-null  int64 \n",
            " 13  DIANOM     68205 non-null  object\n",
            " 14  TIPOVUELO  68205 non-null  object\n",
            " 15  OPERA      68205 non-null  object\n",
            " 16  SIGLAORI   68205 non-null  object\n",
            " 17  SIGLADES   68205 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 9.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "    print(i)\n",
        "    print('Unique Values: '+str(len(df.groupby([i]).count())))\n",
        "    print(df[i].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01dRNfeoAsWJ",
        "outputId": "addf9bd0-c672-4db7-ba98-82812c0b7b99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fecha-I\n",
            "Unique Values: 53252\n",
            "2017-07-28 13:30:00    6\n",
            "2017-03-19 18:00:00    6\n",
            "2017-03-26 18:00:00    6\n",
            "2017-12-16 13:35:00    5\n",
            "2017-02-15 13:35:00    5\n",
            "                      ..\n",
            "2017-06-01 15:16:00    1\n",
            "2017-06-02 15:16:00    1\n",
            "2017-06-03 15:16:00    1\n",
            "2017-06-04 15:16:00    1\n",
            "2017-12-31 14:55:00    1\n",
            "Name: Fecha-I, Length: 53252, dtype: int64\n",
            "Vlo-I\n",
            "Unique Values: 750\n",
            "174     686\n",
            "11      645\n",
            "116     608\n",
            "150     557\n",
            "162     553\n",
            "       ... \n",
            "1158      1\n",
            "368       1\n",
            "9955      1\n",
            "9701      1\n",
            "1218      1\n",
            "Name: Vlo-I, Length: 750, dtype: int64\n",
            "Ori-I\n",
            "Unique Values: 1\n",
            "SCEL    68205\n",
            "Name: Ori-I, dtype: int64\n",
            "Des-I\n",
            "Unique Values: 64\n",
            "SCFA    5787\n",
            "SPJC    5269\n",
            "SCCF    5145\n",
            "SCTE    4357\n",
            "SCIE    3995\n",
            "        ... \n",
            "SBFI       1\n",
            "SPSO       1\n",
            "SEQU       1\n",
            "SEQM       1\n",
            "SARI       1\n",
            "Name: Des-I, Length: 64, dtype: int64\n",
            "Emp-I\n",
            "Unique Values: 30\n",
            "LAN    37611\n",
            "SKU    14298\n",
            "TAM     3049\n",
            "ARG     1949\n",
            "CMP     1850\n",
            "LAW     1573\n",
            "AVA     1152\n",
            "JAT     1095\n",
            "GLO      806\n",
            "AAL      757\n",
            "ACA      565\n",
            "IBE      362\n",
            "AFR      358\n",
            "DAL      358\n",
            "AMX      351\n",
            "UAL      335\n",
            "ONE      279\n",
            "AZA      259\n",
            "KLM      251\n",
            "LAP      216\n",
            "BAW      205\n",
            "QFU      195\n",
            "JMR      100\n",
            "LRC       92\n",
            "AUT       74\n",
            "PUE       49\n",
            "LXP        9\n",
            "LPE        4\n",
            "DSM        2\n",
            "LNE        1\n",
            "Name: Emp-I, dtype: int64\n",
            "Fecha-O\n",
            "Unique Values: 62774\n",
            "2017-05-19 07:01:00    5\n",
            "2017-11-05 14:51:00    5\n",
            "2017-10-25 07:37:00    4\n",
            "2017-03-08 13:34:00    4\n",
            "2017-09-07 20:10:00    4\n",
            "                      ..\n",
            "2017-05-04 15:48:00    1\n",
            "2017-05-05 17:02:00    1\n",
            "2017-05-07 17:03:00    1\n",
            "2017-05-08 17:06:00    1\n",
            "2017-12-31 15:04:00    1\n",
            "Name: Fecha-O, Length: 62774, dtype: int64\n",
            "Vlo-O\n",
            "Unique Values: 866\n",
            "174      649\n",
            "11       646\n",
            "116      608\n",
            "150      517\n",
            "704      514\n",
            "        ... \n",
            "356        1\n",
            "1148       1\n",
            "846A       1\n",
            "4950       1\n",
            "180.0      1\n",
            "Name: Vlo-O, Length: 866, dtype: int64\n",
            "Ori-O\n",
            "Unique Values: 1\n",
            "SCEL    68205\n",
            "Name: Ori-O, dtype: int64\n",
            "Des-O\n",
            "Unique Values: 63\n",
            "SCFA    5786\n",
            "SPJC    5269\n",
            "SCCF    5146\n",
            "SCTE    4357\n",
            "SCIE    3993\n",
            "        ... \n",
            "SEQM       2\n",
            "KIAD       1\n",
            "SPSO       1\n",
            "EGYP       1\n",
            "SLCB       1\n",
            "Name: Des-O, Length: 63, dtype: int64\n",
            "Emp-O\n",
            "Unique Values: 32\n",
            "LAN    20988\n",
            "LXP    14558\n",
            "SKU    14298\n",
            "TAM     3046\n",
            "ARG     1946\n",
            "CMP     1850\n",
            "JMR     1647\n",
            "LPE     1214\n",
            "JAT     1095\n",
            "AVA      885\n",
            "GLO      806\n",
            "AAL      757\n",
            "ACA      565\n",
            "DSM      493\n",
            "LNE      374\n",
            "IBE      362\n",
            "AFR      358\n",
            "DAL      358\n",
            "LRC      357\n",
            "AMX      351\n",
            "UAL      335\n",
            "ONE      279\n",
            "AZA      259\n",
            "KLM      251\n",
            "LAP      219\n",
            "BAW      205\n",
            "QFA      195\n",
            "AUT       77\n",
            "PUE       49\n",
            "56R       16\n",
            "48O       10\n",
            "TPU        2\n",
            "Name: Emp-O, dtype: int64\n",
            "DIA\n",
            "Unique Values: 31\n",
            "20    2290\n",
            "27    2286\n",
            "12    2284\n",
            "10    2283\n",
            "6     2275\n",
            "22    2272\n",
            "13    2272\n",
            "3     2271\n",
            "16    2268\n",
            "26    2267\n",
            "21    2267\n",
            "7     2264\n",
            "23    2260\n",
            "5     2241\n",
            "15    2239\n",
            "11    2237\n",
            "19    2237\n",
            "24    2232\n",
            "28    2232\n",
            "17    2228\n",
            "9     2227\n",
            "2     2223\n",
            "14    2221\n",
            "4     2215\n",
            "8     2213\n",
            "1     2208\n",
            "25    2179\n",
            "18    2160\n",
            "29    2044\n",
            "30    2020\n",
            "31    1290\n",
            "Name: DIA, dtype: int64\n",
            "MES\n",
            "Unique Values: 12\n",
            "12    6356\n",
            "1     6107\n",
            "11    6080\n",
            "10    6032\n",
            "7     5992\n",
            "8     5744\n",
            "9     5610\n",
            "2     5561\n",
            "3     5482\n",
            "5     5240\n",
            "4     5020\n",
            "6     4981\n",
            "Name: MES, dtype: int64\n",
            "AÑO\n",
            "Unique Values: 2\n",
            "2017    68203\n",
            "2018        2\n",
            "Name: AÑO, dtype: int64\n",
            "DIANOM\n",
            "Unique Values: 7\n",
            "Viernes      10292\n",
            "Jueves       10250\n",
            "Lunes        10131\n",
            "Domingo       9796\n",
            "Miercoles     9722\n",
            "Martes        9662\n",
            "Sabado        8352\n",
            "Name: DIANOM, dtype: int64\n",
            "TIPOVUELO\n",
            "Unique Values: 2\n",
            "N    36966\n",
            "I    31239\n",
            "Name: TIPOVUELO, dtype: int64\n",
            "OPERA\n",
            "Unique Values: 23\n",
            "Grupo LATAM                 40892\n",
            "Sky Airline                 14298\n",
            "Aerolineas Argentinas        1949\n",
            "Copa Air                     1850\n",
            "Latin American Wings         1673\n",
            "Avianca                      1152\n",
            "JetSmart SPA                 1095\n",
            "Gol Trans                     806\n",
            "American Airlines             757\n",
            "Air Canada                    565\n",
            "Iberia                        362\n",
            "Delta Air                     358\n",
            "Air France                    358\n",
            "Aeromexico                    351\n",
            "United Airlines               335\n",
            "Oceanair Linhas Aereas        279\n",
            "Alitalia                      259\n",
            "K.L.M.                        251\n",
            "British Airways               205\n",
            "Qantas Airways                195\n",
            "Lacsa                          92\n",
            "Austral                        74\n",
            "Plus Ultra Lineas Aereas       49\n",
            "Name: OPERA, dtype: int64\n",
            "SIGLAORI\n",
            "Unique Values: 1\n",
            "Santiago    68205\n",
            "Name: SIGLAORI, dtype: int64\n",
            "SIGLADES\n",
            "Unique Values: 62\n",
            "Buenos Aires      6335\n",
            "Antofagasta       5786\n",
            "Lima              5269\n",
            "Calama            5146\n",
            "Puerto Montt      4357\n",
            "                  ... \n",
            "Quito                2\n",
            "Washington           1\n",
            "Pisco, Peru          1\n",
            "Puerto Stanley       1\n",
            "Cochabamba           1\n",
            "Name: SIGLADES, Length: 62, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new categorical column based on Column1\n",
        "df['TIPOVUELO_int'] = df['TIPOVUELO'].apply(lambda x: 1 if x == 'I' else 0)\n",
        "\n",
        "# Print the updated dataframe\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6knD0dim1o",
        "outputId": "77548a7e-5e19-4fbd-debe-229ec41a5a71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Fecha-I        68205 non-null  object\n",
            " 1   Vlo-I          68205 non-null  object\n",
            " 2   Ori-I          68205 non-null  object\n",
            " 3   Des-I          68205 non-null  object\n",
            " 4   Emp-I          68205 non-null  object\n",
            " 5   Fecha-O        68205 non-null  object\n",
            " 6   Vlo-O          68205 non-null  object\n",
            " 7   Ori-O          68205 non-null  object\n",
            " 8   Des-O          68205 non-null  object\n",
            " 9   Emp-O          68205 non-null  object\n",
            " 10  DIA            68205 non-null  int64 \n",
            " 11  MES            68205 non-null  int64 \n",
            " 12  AÑO            68205 non-null  int64 \n",
            " 13  DIANOM         68205 non-null  object\n",
            " 14  TIPOVUELO      68205 non-null  object\n",
            " 15  OPERA          68205 non-null  object\n",
            " 16  SIGLAORI       68205 non-null  object\n",
            " 17  SIGLADES       68205 non-null  object\n",
            " 18  TIPOVUELO_int  68205 non-null  int64 \n",
            "dtypes: int64(4), object(15)\n",
            "memory usage: 10.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creation of feature variables"
      ],
      "metadata": {
        "id": "irdDhtNasx3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your date column is called \"Date-I\"\n",
        "# Create a datetime object from the \"Date-I\" column\n",
        "df['Date-I'] = pd.to_datetime(df['Fecha-I'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Define the high season periods\n",
        "high_season_periods = [\n",
        "    (pd.to_datetime('12-15-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('03-03-2018', format=\"%m-%d-%Y\").date()),\n",
        "    (pd.to_datetime('07-15-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('07-31-2017', format=\"%m-%d-%Y\").date()),\n",
        "    (pd.to_datetime('09-11-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('09-30-2017', format=\"%m-%d-%Y\").date())\n",
        "]\n",
        "\n",
        "# Define a function to check if a date is in a high season period\n",
        "def is_in_high_season(date):\n",
        "    for period in high_season_periods:\n",
        "        if period[0] <= date.date() <= period[1]:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "# Apply the function to create the \"high_season\" column\n",
        "df['high_season'] = df['Date-I'].apply(is_in_high_season)"
      ],
      "metadata": {
        "id": "giUpkya5k7jX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to general instructions, it should be considered landing and takeoff of aircraft but it is not clear according to times if it is landing or taking off, variables are reviewed and it is not possible to conclude if it is landing or taking off without an extra variable. Therefore we proceed to create the function taking into consideration that only time will be subtracted and in case the result is negative it will be consider as negative in order to enter to the predicction."
      ],
      "metadata": {
        "id": "dlL-p3NSPg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Date-I'] = pd.to_datetime(df['Fecha-I'])\n",
        "df['Date-O'] = pd.to_datetime(df['Fecha-O'])\n",
        "\n",
        "# Handle negative values in min_diff column\n",
        "# Calculate the time difference between two columns\n",
        "df['min_diff'] = df.apply(lambda row: (row['Date-O'] - row['Date-I']).total_seconds() / 60 if row['Date-O'] > row['Date-I'] else (row['Date-I'] - row['Date-O']).total_seconds() / -60, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add period_day column based on Date-I\n",
        "df['period_day'] = df['Date-I'].apply(lambda x: 'morning' if 5 <= x.hour < 12 else ('afternoon' if 12 <= x.hour < 19 else 'night'))\n",
        "\n",
        "# Create a function to set delay_15 column based on min_diff\n",
        "def set_delay_15(x):\n",
        "    if x > 15:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Apply the set_delay_15 function to create delay_15 column\n",
        "df['delay_15'] = df['min_diff'].apply(set_delay_15)\n",
        "\n",
        "# Print the updated dataframe\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "861ivnJPtHJp",
        "outputId": "f10c8e5d-6ff7-4c55-8b03-67a2d5892e34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
              "0      2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   \n",
              "1      2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   \n",
              "2      2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   \n",
              "3      2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   \n",
              "4      2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   \n",
              "...                    ...   ...   ...   ...   ...                  ...   \n",
              "68201  2017-12-22 14:55:00   400  SCEL  SPJC   JAT  2017-12-22 15:41:00   \n",
              "68202  2017-12-25 14:55:00   400  SCEL  SPJC   JAT  2017-12-25 15:11:00   \n",
              "68203  2017-12-27 14:55:00   400  SCEL  SPJC   JAT  2017-12-27 15:35:00   \n",
              "68204  2017-12-29 14:55:00   400  SCEL  SPJC   JAT  2017-12-29 15:08:00   \n",
              "68205  2017-12-31 14:55:00   400  SCEL  SPJC   JAT  2017-12-31 15:04:00   \n",
              "\n",
              "       Vlo-O Ori-O Des-O Emp-O  ...              OPERA  SIGLAORI  SIGLADES  \\\n",
              "0        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "1        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "2        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "3        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "4        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "...      ...   ...   ...   ...  ...                ...       ...       ...   \n",
              "68201  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68202  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68203  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68204  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68205  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "\n",
              "      TIPOVUELO_int              Date-I high_season              Date-O  \\\n",
              "0                 1 2017-01-01 23:30:00           0 2017-01-01 23:33:00   \n",
              "1                 1 2017-01-02 23:30:00           0 2017-01-02 23:39:00   \n",
              "2                 1 2017-01-03 23:30:00           0 2017-01-03 23:39:00   \n",
              "3                 1 2017-01-04 23:30:00           0 2017-01-04 23:33:00   \n",
              "4                 1 2017-01-05 23:30:00           0 2017-01-05 23:28:00   \n",
              "...             ...                 ...         ...                 ...   \n",
              "68201             1 2017-12-22 14:55:00           1 2017-12-22 15:41:00   \n",
              "68202             1 2017-12-25 14:55:00           1 2017-12-25 15:11:00   \n",
              "68203             1 2017-12-27 14:55:00           1 2017-12-27 15:35:00   \n",
              "68204             1 2017-12-29 14:55:00           1 2017-12-29 15:08:00   \n",
              "68205             1 2017-12-31 14:55:00           1 2017-12-31 15:04:00   \n",
              "\n",
              "      min_diff  period_day delay_15  \n",
              "0          3.0       night        0  \n",
              "1          9.0       night        0  \n",
              "2          9.0       night        0  \n",
              "3          3.0       night        0  \n",
              "4         -2.0       night        0  \n",
              "...        ...         ...      ...  \n",
              "68201     46.0   afternoon        1  \n",
              "68202     16.0   afternoon        1  \n",
              "68203     40.0   afternoon        1  \n",
              "68204     13.0   afternoon        0  \n",
              "68205      9.0   afternoon        0  \n",
              "\n",
              "[68205 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1a3a9da-ba6a-4364-9f00-b4f821dc1ca3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha-I</th>\n",
              "      <th>Vlo-I</th>\n",
              "      <th>Ori-I</th>\n",
              "      <th>Des-I</th>\n",
              "      <th>Emp-I</th>\n",
              "      <th>Fecha-O</th>\n",
              "      <th>Vlo-O</th>\n",
              "      <th>Ori-O</th>\n",
              "      <th>Des-O</th>\n",
              "      <th>Emp-O</th>\n",
              "      <th>...</th>\n",
              "      <th>OPERA</th>\n",
              "      <th>SIGLAORI</th>\n",
              "      <th>SIGLADES</th>\n",
              "      <th>TIPOVUELO_int</th>\n",
              "      <th>Date-I</th>\n",
              "      <th>high_season</th>\n",
              "      <th>Date-O</th>\n",
              "      <th>min_diff</th>\n",
              "      <th>period_day</th>\n",
              "      <th>delay_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68201</th>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>46.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68202</th>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>16.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68203</th>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>40.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68204</th>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>13.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68205</th>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68205 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a3a9da-ba6a-4364-9f00-b4f821dc1ca3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1a3a9da-ba6a-4364-9f00-b4f821dc1ca3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1a3a9da-ba6a-4364-9f00-b4f821dc1ca3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionary to map string values to integers\n",
        "mapping_dict = {\n",
        "    'Lunes': 0,\n",
        "    'Martes': 1,\n",
        "    'Miercoles': 2,\n",
        "    'Jueves': 3,\n",
        "    'Viernes': 4,\n",
        "    'Sabado': 5,\n",
        "    'Domingo': 6\n",
        "}\n",
        "\n",
        "# create new column using map method with dictionary\n",
        "df['DIANOM_int'] = df['DIANOM'].map(mapping_dict)\n",
        "\n",
        "# add 7 as a possible value (since there are 7 string values)\n",
        "df['DIANOM_int'] = df['DIANOM_int'].fillna(7).astype(int)"
      ],
      "metadata": {
        "id": "0r3MDbuuLpjD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionary to map string values to integers\n",
        "mapping_dict = {\n",
        "    'morning': 0,\n",
        "    'afternoon': 1,\n",
        "    'night': 2,\n",
        "}\n",
        "\n",
        "# create new column using map method with dictionary\n",
        "df['period_day_int'] = df['period_day'].map(mapping_dict)\n",
        "\n",
        "# add 7 as a possible value (since there are 7 string values)\n",
        "df['period_day_int'] = df['period_day_int'].fillna(7).astype(int)"
      ],
      "metadata": {
        "id": "JVKL3ZrLM3O1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var = ['OPERA','SIGLAORI',\"SIGLADES\"]\n",
        "for i in cat_var:\n",
        "   name = i + '_int'\n",
        "   df[name] = pd.factorize(df[i])[0]\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg0vQyCyZtP-",
        "outputId": "ab8093f6-c02e-42f9-f3f6-35fd447a3c7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 30 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   Fecha-I         68205 non-null  object        \n",
            " 1   Vlo-I           68205 non-null  object        \n",
            " 2   Ori-I           68205 non-null  object        \n",
            " 3   Des-I           68205 non-null  object        \n",
            " 4   Emp-I           68205 non-null  object        \n",
            " 5   Fecha-O         68205 non-null  object        \n",
            " 6   Vlo-O           68205 non-null  object        \n",
            " 7   Ori-O           68205 non-null  object        \n",
            " 8   Des-O           68205 non-null  object        \n",
            " 9   Emp-O           68205 non-null  object        \n",
            " 10  DIA             68205 non-null  int64         \n",
            " 11  MES             68205 non-null  int64         \n",
            " 12  AÑO             68205 non-null  int64         \n",
            " 13  DIANOM          68205 non-null  object        \n",
            " 14  TIPOVUELO       68205 non-null  object        \n",
            " 15  OPERA           68205 non-null  object        \n",
            " 16  SIGLAORI        68205 non-null  object        \n",
            " 17  SIGLADES        68205 non-null  object        \n",
            " 18  TIPOVUELO_int   68205 non-null  int64         \n",
            " 19  Date-I          68205 non-null  datetime64[ns]\n",
            " 20  high_season     68205 non-null  int64         \n",
            " 21  Date-O          68205 non-null  datetime64[ns]\n",
            " 22  min_diff        68205 non-null  float64       \n",
            " 23  period_day      68205 non-null  object        \n",
            " 24  delay_15        68205 non-null  int64         \n",
            " 25  DIANOM_int      68205 non-null  int64         \n",
            " 26  period_day_int  68205 non-null  int64         \n",
            " 27  OPERA_int       68205 non-null  int64         \n",
            " 28  SIGLAORI_int    68205 non-null  int64         \n",
            " 29  SIGLADES_int    68205 non-null  int64         \n",
            "dtypes: datetime64[ns](2), float64(1), int64(11), object(16)\n",
            "memory usage: 16.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library and alias it as \"pd\"\n",
        "import pandas as pd\n",
        "\n",
        "# Select only the columns we want to keep in a new DataFrame called \"df_export\"\n",
        "# Here, we assume the original DataFrame is called \"df\"\n",
        "df_export = df[['high_season', 'min_diff', 'delay_15', 'period_day']]\n",
        "\n",
        "# Export the new DataFrame to a CSV file named \"syntethic_feature.csv\"\n",
        "# The \"index=False\" parameter prevents the row index from being written to the file\n",
        "# The \"header=True\" parameter writes the column names to the file\n",
        "df_export.to_csv(\"syntethic_feature.csv\", index=False, header=True)"
      ],
      "metadata": {
        "id": "gtgwL29Q1dDz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outliers treatment"
      ],
      "metadata": {
        "id": "C5_Y2yg5lBms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(df,features):\n",
        "    outlier_indices=[]\n",
        "    \n",
        "    for c in features:\n",
        "        # 1st quartile\n",
        "        Q1=np.percentile(df[c],25)\n",
        "        \n",
        "        # 3rd quartile\n",
        "        Q3=np.percentile(df[c],75)\n",
        "        \n",
        "        # IQR\n",
        "        IQR= Q3-Q1\n",
        "        \n",
        "        # Outlier Step\n",
        "        outlier_step= IQR * 2\n",
        "        \n",
        "        # Detect outlier and their indeces \n",
        "        outlier_list_col = df[(df[c]< Q1 - outlier_step)|( df[c] > Q3 + outlier_step)].index\n",
        "        \n",
        "        # Store indices \n",
        "        outlier_indices.extend(outlier_list_col)\n",
        "    \n",
        "    outliers_indices = Counter(outlier_indices)\n",
        "    multiple_outliers = list(i for i , v in outliers_indices.items() if v>2 )\n",
        "    return multiple_outliers"
      ],
      "metadata": {
        "id": "cTpYixhwlCyf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from collections import Counter\n",
        "\n",
        "df.loc[detect_outliers(df,['DIA',\n",
        " 'MES',\n",
        " 'AÑO',\n",
        " 'TIPOVUELO_int',\n",
        " 'TIPOVUELO_int',\n",
        " 'Date-I',\n",
        " 'high_season',\n",
        " 'Date-O',\n",
        " 'min_diff',\n",
        " 'delay_15',\n",
        " 'DIANOM_int',\n",
        " 'period_day_int',\n",
        " 'OPERA_int',\n",
        " 'SIGLAORI_int',\n",
        " 'SIGLADES_int'])]"
      ],
      "metadata": {
        "id": "5KTQDDmYlIDI",
        "outputId": "a9c325e8-b101-49b4-f65c-bd6f4b98cb55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
              "66947  2017-12-31 23:40:00   622  SCEL  MMMX   LAN  2018-01-01 00:12:00   \n",
              "32405  2017-07-15 20:00:00   912  SCEL  KMIA   AAL  2017-07-15 21:59:00   \n",
              "32410  2017-07-21 20:00:00   912  SCEL  KMIA   AAL  2017-07-21 20:17:00   \n",
              "32437  2017-07-17 20:30:00   940  SCEL  KDFW   AAL  2017-07-17 20:55:00   \n",
              "32441  2017-07-21 20:30:00   940  SCEL  KDFW   AAL  2017-07-21 22:01:00   \n",
              "...                    ...   ...   ...   ...   ...                  ...   \n",
              "62707  2017-12-10 00:55:00  8525  SCEL  SBGR   ONE  2017-12-10 01:38:00   \n",
              "67996  2017-12-04 13:27:00    50  SCEL  SCSE   JAT  2017-12-04 14:19:00   \n",
              "68081  2017-12-06 16:21:00   201  SCEL  SCIE   JAT  2017-12-06 17:02:00   \n",
              "68138  2017-12-08 15:06:00   253  SCEL  SCTE   JAT  2017-12-08 15:49:00   \n",
              "68161  2017-12-06 11:57:00   281  SCEL  SCQP   JAT  2017-12-06 13:00:00   \n",
              "\n",
              "       Vlo-O Ori-O Des-O Emp-O  ...  high_season              Date-O  \\\n",
              "66947  622.0  SCEL  MMMX   LAN  ...            1 2018-01-01 00:12:00   \n",
              "32405    912  SCEL  KMIA   AAL  ...            1 2017-07-15 21:59:00   \n",
              "32410    912  SCEL  KMIA   AAL  ...            1 2017-07-21 20:17:00   \n",
              "32437    940  SCEL  KDFW   AAL  ...            1 2017-07-17 20:55:00   \n",
              "32441    940  SCEL  KDFW   AAL  ...            1 2017-07-21 22:01:00   \n",
              "...      ...   ...   ...   ...  ...          ...                 ...   \n",
              "62707   8525  SCEL  SBGR   ONE  ...            0 2017-12-10 01:38:00   \n",
              "67996   50.0  SCEL  SCSE   JAT  ...            0 2017-12-04 14:19:00   \n",
              "68081  201.0  SCEL  SCIE   JAT  ...            0 2017-12-06 17:02:00   \n",
              "68138  253.0  SCEL  SCTE   JAT  ...            0 2017-12-08 15:49:00   \n",
              "68161  281.0  SCEL  SCQP   JAT  ...            0 2017-12-06 13:00:00   \n",
              "\n",
              "       min_diff period_day delay_15 DIANOM_int period_day_int OPERA_int  \\\n",
              "66947      32.0      night        1          0              2        16   \n",
              "32405     119.0      night        1          5              2         0   \n",
              "32410      17.0      night        1          4              2         0   \n",
              "32437      25.0      night        1          0              2         0   \n",
              "32441      91.0      night        1          4              2         0   \n",
              "...         ...        ...      ...        ...            ...       ...   \n",
              "62707      43.0      night        1          6              2        21   \n",
              "67996      52.0  afternoon        1          0              1        20   \n",
              "68081      41.0  afternoon        1          2              1        20   \n",
              "68138      43.0  afternoon        1          4              1        20   \n",
              "68161      63.0    morning        1          2              0        20   \n",
              "\n",
              "       SIGLAORI_int SIGLADES_int  \n",
              "66947             0            5  \n",
              "32405             0            0  \n",
              "32410             0            0  \n",
              "32437             0            1  \n",
              "32441             0            1  \n",
              "...             ...          ...  \n",
              "62707             0           11  \n",
              "67996             0           27  \n",
              "68081             0           26  \n",
              "68138             0           20  \n",
              "68161             0           24  \n",
              "\n",
              "[1627 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdf0a935-aade-42b0-8d21-35e4f4b215a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha-I</th>\n",
              "      <th>Vlo-I</th>\n",
              "      <th>Ori-I</th>\n",
              "      <th>Des-I</th>\n",
              "      <th>Emp-I</th>\n",
              "      <th>Fecha-O</th>\n",
              "      <th>Vlo-O</th>\n",
              "      <th>Ori-O</th>\n",
              "      <th>Des-O</th>\n",
              "      <th>Emp-O</th>\n",
              "      <th>...</th>\n",
              "      <th>high_season</th>\n",
              "      <th>Date-O</th>\n",
              "      <th>min_diff</th>\n",
              "      <th>period_day</th>\n",
              "      <th>delay_15</th>\n",
              "      <th>DIANOM_int</th>\n",
              "      <th>period_day_int</th>\n",
              "      <th>OPERA_int</th>\n",
              "      <th>SIGLAORI_int</th>\n",
              "      <th>SIGLADES_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>66947</th>\n",
              "      <td>2017-12-31 23:40:00</td>\n",
              "      <td>622</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>MMMX</td>\n",
              "      <td>LAN</td>\n",
              "      <td>2018-01-01 00:12:00</td>\n",
              "      <td>622.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>MMMX</td>\n",
              "      <td>LAN</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2018-01-01 00:12:00</td>\n",
              "      <td>32.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32405</th>\n",
              "      <td>2017-07-15 20:00:00</td>\n",
              "      <td>912</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-07-15 21:59:00</td>\n",
              "      <td>912</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-07-15 21:59:00</td>\n",
              "      <td>119.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32410</th>\n",
              "      <td>2017-07-21 20:00:00</td>\n",
              "      <td>912</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-07-21 20:17:00</td>\n",
              "      <td>912</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-07-21 20:17:00</td>\n",
              "      <td>17.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32437</th>\n",
              "      <td>2017-07-17 20:30:00</td>\n",
              "      <td>940</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-07-17 20:55:00</td>\n",
              "      <td>940</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-07-17 20:55:00</td>\n",
              "      <td>25.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32441</th>\n",
              "      <td>2017-07-21 20:30:00</td>\n",
              "      <td>940</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-07-21 22:01:00</td>\n",
              "      <td>940</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-07-21 22:01:00</td>\n",
              "      <td>91.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62707</th>\n",
              "      <td>2017-12-10 00:55:00</td>\n",
              "      <td>8525</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SBGR</td>\n",
              "      <td>ONE</td>\n",
              "      <td>2017-12-10 01:38:00</td>\n",
              "      <td>8525</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SBGR</td>\n",
              "      <td>ONE</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-12-10 01:38:00</td>\n",
              "      <td>43.0</td>\n",
              "      <td>night</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67996</th>\n",
              "      <td>2017-12-04 13:27:00</td>\n",
              "      <td>50</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCSE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-04 14:19:00</td>\n",
              "      <td>50.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCSE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-12-04 14:19:00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68081</th>\n",
              "      <td>2017-12-06 16:21:00</td>\n",
              "      <td>201</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCIE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-06 17:02:00</td>\n",
              "      <td>201.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCIE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-12-06 17:02:00</td>\n",
              "      <td>41.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68138</th>\n",
              "      <td>2017-12-08 15:06:00</td>\n",
              "      <td>253</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCTE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-08 15:49:00</td>\n",
              "      <td>253.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCTE</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-12-08 15:49:00</td>\n",
              "      <td>43.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68161</th>\n",
              "      <td>2017-12-06 11:57:00</td>\n",
              "      <td>281</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCQP</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-06 13:00:00</td>\n",
              "      <td>281.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SCQP</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-12-06 13:00:00</td>\n",
              "      <td>63.0</td>\n",
              "      <td>morning</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1627 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdf0a935-aade-42b0-8d21-35e4f4b215a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdf0a935-aade-42b0-8d21-35e4f4b215a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdf0a935-aade-42b0-8d21-35e4f4b215a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(detect_outliers(df,['DIA',\n",
        " 'MES',\n",
        " 'AÑO',\n",
        " 'TIPOVUELO_int',\n",
        " 'TIPOVUELO_int',\n",
        " 'Date-I',\n",
        " 'high_season',\n",
        " 'Date-O',\n",
        " 'min_diff',\n",
        " 'delay_15',\n",
        " 'DIANOM_int',\n",
        " 'period_day_int',\n",
        " 'OPERA_int',\n",
        " 'SIGLAORI_int',\n",
        " 'SIGLADES_int']),axis = 0).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "ihrBB8TqlRAf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph analisis and ratios"
      ],
      "metadata": {
        "id": "SS1AEmFMBA2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected = df[['DIA', 'MES', 'AÑO', 'TIPOVUELO', 'high_season', 'min_diff', 'delay_15', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES']]\n"
      ],
      "metadata": {
        "id": "vVVp7FfD7bgS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "CPKXgvlL-KAO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# select the relevant columns for the analysis\n",
        "df_selected = train_df[['DIA', 'MES', 'AÑO', 'TIPOVUELO', 'high_season', 'min_diff', 'delay_15', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES']]\n",
        "\n",
        "# convert categorical variables into binary variables\n",
        "df_binary = pd.get_dummies(df_selected, columns=['TIPOVUELO', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES'], drop_first=True)\n",
        "\n",
        "# concatenate the original DataFrame and the binary DataFrame\n",
        "train_df_with_dummies = pd.concat([train_df, df_binary], axis=1)\n",
        "\n",
        "# print the resulting DataFrame\n",
        "print(train_df_with_dummies)"
      ],
      "metadata": {
        "id": "cezFdE9OnxB0",
        "outputId": "fbee37b3-8274-4f13-abd7-9c1a88f5d87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O Vlo-O  \\\n",
            "34475  2017-07-18 02:00:00    97  SCEL  SCCI   LAN  2017-07-18 02:03:00    97   \n",
            "38612  2017-08-13 19:30:00  8069  SCEL  SBGR   TAM  2017-08-13 19:51:00  8069   \n",
            "28653  2017-06-21 19:05:00   116  SCEL  SCAT   SKU  2017-06-21 19:03:00   116   \n",
            "58409  2017-11-01 12:50:00   302  SCEL  SCSE   LAN  2017-11-01 12:49:00   302   \n",
            "10873  2017-02-04 08:30:00   750  SCEL  SBGR   LAN  2017-02-04 08:44:00   750   \n",
            "...                    ...   ...   ...   ...   ...                  ...   ...   \n",
            "37194  2017-07-06 05:35:00   843  SCEL  SCIP   LAN  2017-07-06 06:51:00   843   \n",
            "6265   2017-02-16 16:45:00  1285  SCEL  SABE   ARG  2017-02-16 16:36:00  1285   \n",
            "54886  2017-11-18 07:15:00  1289  SCEL  SABE   ARG  2017-11-18 07:08:00  1289   \n",
            "860    2017-01-07 12:15:00  8021  SCEL  SBGL   TAM  2017-01-07 12:16:00  8021   \n",
            "15795  2017-03-08 15:35:00   445  SCEL  SAEZ   LAN  2017-03-08 15:29:00   445   \n",
            "\n",
            "      Ori-O Des-O Emp-O  ...  SIGLADES_San Juan, Arg.  SIGLADES_Santa Cruz  \\\n",
            "34475  SCEL  SCCI   LAN  ...                        0                    0   \n",
            "38612  SCEL  SBGR   TAM  ...                        0                    0   \n",
            "28653  SCEL  SCAT   SKU  ...                        0                    0   \n",
            "58409  SCEL  SCSE   LXP  ...                        0                    0   \n",
            "10873  SCEL  SBGR   LAN  ...                        0                    0   \n",
            "...     ...   ...   ...  ...                      ...                  ...   \n",
            "37194  SCEL  SCIP   LAN  ...                        0                    0   \n",
            "6265   SCEL  SABE   ARG  ...                        0                    0   \n",
            "54886  SCEL  SABE   ARG  ...                        0                    0   \n",
            "860    SCEL  SBGL   TAM  ...                        0                    0   \n",
            "15795  SCEL  SAEZ   LAN  ...                        0                    0   \n",
            "\n",
            "       SIGLADES_Sao Paulo SIGLADES_Sydney SIGLADES_Temuco SIGLADES_Toronto  \\\n",
            "34475                   0               0               0                0   \n",
            "38612                   1               0               0                0   \n",
            "28653                   0               0               0                0   \n",
            "58409                   0               0               0                0   \n",
            "10873                   1               0               0                0   \n",
            "...                   ...             ...             ...              ...   \n",
            "37194                   0               0               0                0   \n",
            "6265                    0               0               0                0   \n",
            "54886                   0               0               0                0   \n",
            "860                     0               0               0                0   \n",
            "15795                   0               0               0                0   \n",
            "\n",
            "      SIGLADES_Tucuman SIGLADES_Ushuia  SIGLADES_Valdivia SIGLADES_Washington  \n",
            "34475                0               0                  0                   0  \n",
            "38612                0               0                  0                   0  \n",
            "28653                0               0                  0                   0  \n",
            "58409                0               0                  0                   0  \n",
            "10873                0               0                  0                   0  \n",
            "...                ...             ...                ...                 ...  \n",
            "37194                0               0                  0                   0  \n",
            "6265                 0               0                  0                   0  \n",
            "54886                0               0                  0                   0  \n",
            "860                  0               0                  0                   0  \n",
            "15795                0               0                  0                   0  \n",
            "\n",
            "[46604 rows x 127 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# select the relevant columns for the analysis\n",
        "df_selected = test_df[['DIA', 'MES', 'AÑO', 'TIPOVUELO', 'high_season', 'min_diff', 'delay_15', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES']]\n",
        "\n",
        "# convert categorical variables into binary variables\n",
        "df_binary = pd.get_dummies(df_selected, columns=['TIPOVUELO', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES'], drop_first=True)\n",
        "\n",
        "# concatenate the original DataFrame and the binary DataFrame\n",
        "test_df_with_dummies = pd.concat([test_df, df_binary], axis=1)\n",
        "\n",
        "# print the resulting DataFrame\n",
        "print(test_df_with_dummies)"
      ],
      "metadata": {
        "id": "sKZ9yViKnx0q",
        "outputId": "9fcfb8e7-49b8-4a3a-e91d-16d9a2cff637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
            "59036  2017-11-14 11:30:00   434  SCEL  SAME   LAN  2017-11-14 11:32:00   \n",
            "31867  2017-06-02 04:15:00  2370  SCEL  SPJC   LAN  2017-06-02 04:15:00   \n",
            "62245  2017-12-07 15:55:00   126  SCEL  SCCF   SKU  2017-12-07 16:00:00   \n",
            "58277  2017-11-30 19:41:00   283  SCEL  SCCI   LAN  2017-11-30 19:45:00   \n",
            "63558  2017-12-16 18:52:00   168  SCEL  SCDA   LAN  2017-12-16 19:24:00   \n",
            "...                    ...   ...   ...   ...   ...                  ...   \n",
            "2348   2017-01-31 18:55:00   531  SCEL  SACO   SKU  2017-01-31 18:59:00   \n",
            "62797  2017-12-23 18:25:00   531  SCEL  SACO   SKU  2017-12-23 19:02:00   \n",
            "60682  2017-12-03 22:30:00   940  SCEL  KDFW   AAL  2017-12-03 22:28:00   \n",
            "6605   2017-02-18 21:50:00   146  SCEL  KATL   DAL  2017-02-18 21:46:00   \n",
            "32595  2017-07-29 11:02:00   118  SCEL  MPTO   CMP  2017-07-29 10:51:00   \n",
            "\n",
            "       Vlo-O Ori-O Des-O Emp-O  ...  SIGLADES_Rosario  \\\n",
            "59036    434  SCEL  SAME   LAN  ...                 0   \n",
            "31867   2370  SCEL  SPJC   LPE  ...                 0   \n",
            "62245    126  SCEL  SCCF   SKU  ...                 0   \n",
            "58277    283  SCEL  SCCI   LXP  ...                 0   \n",
            "63558  168.0  SCEL  SCDA   LAN  ...                 0   \n",
            "...      ...   ...   ...   ...  ...               ...   \n",
            "2348     531  SCEL  SACO   SKU  ...                 0   \n",
            "62797  531.0  SCEL  SACO   SKU  ...                 0   \n",
            "60682    940  SCEL  KDFW   AAL  ...                 0   \n",
            "6605     146  SCEL  KATL   DAL  ...                 0   \n",
            "32595    118  SCEL  MPTO   CMP  ...                 0   \n",
            "\n",
            "       SIGLADES_San Juan, Arg.  SIGLADES_Santa Cruz SIGLADES_Sao Paulo  \\\n",
            "59036                        0                    0                  0   \n",
            "31867                        0                    0                  0   \n",
            "62245                        0                    0                  0   \n",
            "58277                        0                    0                  0   \n",
            "63558                        0                    0                  0   \n",
            "...                        ...                  ...                ...   \n",
            "2348                         0                    0                  0   \n",
            "62797                        0                    0                  0   \n",
            "60682                        0                    0                  0   \n",
            "6605                         0                    0                  0   \n",
            "32595                        0                    0                  0   \n",
            "\n",
            "      SIGLADES_Sydney SIGLADES_Temuco SIGLADES_Toronto SIGLADES_Tucuman  \\\n",
            "59036               0               0                0                0   \n",
            "31867               0               0                0                0   \n",
            "62245               0               0                0                0   \n",
            "58277               0               0                0                0   \n",
            "63558               0               0                0                0   \n",
            "...               ...             ...              ...              ...   \n",
            "2348                0               0                0                0   \n",
            "62797               0               0                0                0   \n",
            "60682               0               0                0                0   \n",
            "6605                0               0                0                0   \n",
            "32595               0               0                0                0   \n",
            "\n",
            "       SIGLADES_Ushuia SIGLADES_Valdivia  \n",
            "59036                0                 0  \n",
            "31867                0                 0  \n",
            "62245                0                 0  \n",
            "58277                0                 0  \n",
            "63558                0                 0  \n",
            "...                ...               ...  \n",
            "2348                 0                 0  \n",
            "62797                0                 0  \n",
            "60682                0                 0  \n",
            "6605                 0                 0  \n",
            "32595                0                 0  \n",
            "\n",
            "[19974 rows x 123 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pdee\n",
        "\n",
        "# select the relevant columns for the analysis\n",
        "df_selected = train_df[['DIA', 'MES', 'AÑO', 'TIPOVUELO', 'high_season', 'min_diff', 'delay_15', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES']]\n",
        "\n",
        "# convert categorical variables into binary variables\n",
        "df_binary = pd.get_dummies(df_selected, columns=['TIPOVUELO', 'DIANOM', 'period_day', 'OPERA', 'SIGLAORI', 'SIGLADES'])\n",
        "\n",
        "# calculate the mean of each variable\n",
        "means = df_binary.mean()\n",
        "\n",
        "# calculate the rate between `min_diff` and each of the other variables\n",
        "rates = pd.DataFrame({\n",
        "    col: df_selected['min_diff'] / means[col] for col in df_binary.columns if col != 'min_diff'\n",
        "})\n",
        "\n",
        "# add a suffix to the column names in the rates DataFrame\n",
        "rates = rates.add_suffix('_rate')\n",
        "\n",
        "# concatenate the original DataFrame and the rates DataFrame\n",
        "train_df_with_rates = pd.concat([train_df, rates], axis=1)\n",
        "\n",
        "# print the resulting DataFrame\n",
        "print(train_df_with_rates)\n"
      ],
      "metadata": {
        "id": "ZJg1HX_o7Kik",
        "outputId": "18ab3379-13ab-4857-eb8c-5ebd6be54ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O Vlo-O  \\\n",
            "34475  2017-07-18 02:00:00    97  SCEL  SCCI   LAN  2017-07-18 02:03:00    97   \n",
            "38612  2017-08-13 19:30:00  8069  SCEL  SBGR   TAM  2017-08-13 19:51:00  8069   \n",
            "28653  2017-06-21 19:05:00   116  SCEL  SCAT   SKU  2017-06-21 19:03:00   116   \n",
            "58409  2017-11-01 12:50:00   302  SCEL  SCSE   LAN  2017-11-01 12:49:00   302   \n",
            "10873  2017-02-04 08:30:00   750  SCEL  SBGR   LAN  2017-02-04 08:44:00   750   \n",
            "...                    ...   ...   ...   ...   ...                  ...   ...   \n",
            "37194  2017-07-06 05:35:00   843  SCEL  SCIP   LAN  2017-07-06 06:51:00   843   \n",
            "6265   2017-02-16 16:45:00  1285  SCEL  SABE   ARG  2017-02-16 16:36:00  1285   \n",
            "54886  2017-11-18 07:15:00  1289  SCEL  SABE   ARG  2017-11-18 07:08:00  1289   \n",
            "860    2017-01-07 12:15:00  8021  SCEL  SBGL   TAM  2017-01-07 12:16:00  8021   \n",
            "15795  2017-03-08 15:35:00   445  SCEL  SAEZ   LAN  2017-03-08 15:29:00   445   \n",
            "\n",
            "      Ori-O Des-O Emp-O  ...  SIGLADES_San Juan, Arg.  SIGLADES_Santa Cruz  \\\n",
            "34475  SCEL  SCCI   LAN  ...                        0                    0   \n",
            "38612  SCEL  SBGR   TAM  ...                        0                    0   \n",
            "28653  SCEL  SCAT   SKU  ...                        0                    0   \n",
            "58409  SCEL  SCSE   LXP  ...                        0                    0   \n",
            "10873  SCEL  SBGR   LAN  ...                        0                    0   \n",
            "...     ...   ...   ...  ...                      ...                  ...   \n",
            "37194  SCEL  SCIP   LAN  ...                        0                    0   \n",
            "6265   SCEL  SABE   ARG  ...                        0                    0   \n",
            "54886  SCEL  SABE   ARG  ...                        0                    0   \n",
            "860    SCEL  SBGL   TAM  ...                        0                    0   \n",
            "15795  SCEL  SAEZ   LAN  ...                        0                    0   \n",
            "\n",
            "       SIGLADES_Sao Paulo SIGLADES_Sydney SIGLADES_Temuco SIGLADES_Toronto  \\\n",
            "34475                   0               0               0                0   \n",
            "38612                   1               0               0                0   \n",
            "28653                   0               0               0                0   \n",
            "58409                   0               0               0                0   \n",
            "10873                   1               0               0                0   \n",
            "...                   ...             ...             ...              ...   \n",
            "37194                   0               0               0                0   \n",
            "6265                    0               0               0                0   \n",
            "54886                   0               0               0                0   \n",
            "860                     0               0               0                0   \n",
            "15795                   0               0               0                0   \n",
            "\n",
            "      SIGLADES_Tucuman SIGLADES_Ushuia  SIGLADES_Valdivia SIGLADES_Washington  \n",
            "34475                0               0                  0                   0  \n",
            "38612                0               0                  0                   0  \n",
            "28653                0               0                  0                   0  \n",
            "58409                0               0                  0                   0  \n",
            "10873                0               0                  0                   0  \n",
            "...                ...             ...                ...                 ...  \n",
            "37194                0               0                  0                   0  \n",
            "6265                 0               0                  0                   0  \n",
            "54886                0               0                  0                   0  \n",
            "860                  0               0                  0                   0  \n",
            "15795                0               0                  0                   0  \n",
            "\n",
            "[46604 rows x 127 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate descriptive statistics for the rate columns\n",
        "rate_stats = train_df_with_rates.filter(regex='_rate$').describe()\n",
        "\n",
        "# print the resulting DataFrame\n",
        "print(rate_stats)\n",
        "# select columns that have suffix '_rate'\n",
        "rate_columns = [col for col in train_df_with_rates.columns if col.endswith('_rate')]\n",
        "\n",
        "# create a new dataframe with only the rate columns\n",
        "df_rates = train_df_with_rates[rate_columns]\n"
      ],
      "metadata": {
        "id": "-dAjI-QF9G9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns that have suffix '_rate' in the train data\n",
        "rate_columns = [col for col in train_df_with_rates.columns if col.endswith('_rate')]\n",
        "\n",
        "# create empty rate columns in the test data with the same names as the train data\n",
        "for col in rate_columns:\n",
        "    test_df[col] = np.nan\n",
        "\n",
        "# calculate descriptive statistics for the rate columns in the train data\n",
        "rate_stats = train_df_with_rates.filter(regex='_rate$').describe()\n",
        "\n",
        "# get the mean values for each rate column\n",
        "rate_means = rate_stats.loc['mean']\n",
        "\n",
        "# replace missing values in the test data with the corresponding mean value for each rate column\n",
        "test_df[rate_columns] = test_df[rate_columns].fillna(rate_means)"
      ],
      "metadata": {
        "id": "tKmpC5y4bx2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate descriptive statistics for the rate columns in the train data\n",
        "rate_stats = train_df_with_rates.filter(regex='_rate$').describe()\n",
        "\n",
        "# get the mean values for each rate column\n",
        "rate_means = rate_stats.loc['mean']\n",
        "\n",
        "# select columns that have suffix '_rate'\n",
        "rate_columns = [col for col in train_df_with_rates.columns if col.endswith('_rate')]\n",
        "\n",
        "# create a new dataframe with only the rate columns in the test data\n",
        "test_rates = test_df[rate_columns]\n",
        "\n",
        "# replace missing values in the test data with the corresponding mean value for each rate column\n",
        "test_rates = test_rates.fillna(rate_means)\n",
        "\n",
        "# merge the test_rates dataframe with the test_df dataframe\n",
        "test_df_with_rates = pd.concat([test_df, test_rates], axis=1)"
      ],
      "metadata": {
        "id": "dHPMgnSbaYbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio between High season and delay\n",
        "count = df[(df['delay_15'] == 1) & (df['high_season'] == 1)].shape[0]\n",
        "count_2 = df[(df['high_season'] == 1)].shape[0]\n",
        "ratio = count / count_2\n",
        "count_3 = df[(df['delay_15'] == 1) & (df['high_season'] == 0)].shape[0]\n",
        "count_4 = df[(df['high_season'] == 0)].shape[0]\n",
        "ratio_2 = count_3 / count_4\n",
        "\n",
        "print ('Ratio referring to high season and delay:' )\n",
        "print (ratio)\n",
        "\n",
        "print ('Ratio referring to low season and delay:' )\n",
        "print (ratio_2)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lpssmuy98AG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio between High season and delay\n",
        "count = df[(df['delay_15'] == 1) & (df['period_day'] == 'night')].shape[0]\n",
        "count_2 = df[(df['period_day'] == 'night')].shape[0]\n",
        "ratio = count / count_2\n",
        "\n",
        "count_3 = df[(df['delay_15'] == 1) & (df['period_day'] == 'afternoon')].shape[0]\n",
        "count_4 = df[(df['period_day'] == 'afternoon')].shape[0]\n",
        "ratio_2 = count_3 / count_4\n",
        "\n",
        "count_5 = df[(df['delay_15'] == 1) & (df['period_day'] == 'morning')].shape[0]\n",
        "count_6 = df[(df['period_day'] == 'morning')].shape[0]\n",
        "ratio_3 = count_5 / count_6\n",
        "\n",
        "print ('Ratio referring to period day night delay:' )\n",
        "print (ratio)\n",
        "\n",
        "print ('Ratio referring to period day afternoon delay:' )\n",
        "print (ratio_2)\n",
        "\n",
        "print ('Ratio referring to period day morning delay:' )\n",
        "print (ratio_3)"
      ],
      "metadata": {
        "id": "yYuuRL3cBP2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period_day = df.groupby('delay_15')['period_day'].value_counts()\n",
        "TIPOVUELO = df.groupby('delay_15')['TIPOVUELO'].value_counts()\n",
        "SIGLAORI =  df.groupby('delay_15')['SIGLAORI'].value_counts()\n",
        "SIGLAORI =  df.groupby('delay_15')['SIGLAORI'].value_counts()\n",
        "SIGLADES =  df.groupby('delay_15')['SIGLADES'].value_counts()\n",
        "SIGLADES =  df.groupby('delay_15')['SIGLADES'].value_counts()\n",
        "OPERA =  df.groupby('delay_15')['OPERA'].value_counts()\n",
        "high_season =  df.groupby('delay_15')['high_season'].value_counts()\n",
        "MES = df.groupby('delay_15')['MES'].value_counts()\n",
        "DIA = df.groupby('delay_15')['DIA'].value_counts()\n",
        "AÑO = df.groupby('delay_15')['AÑO'].value_counts()\n",
        "DIANOM = df.groupby('delay_15')['DIANOM'].value_counts()"
      ],
      "metadata": {
        "id": "wDh1J_1mCDqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='SIGLADES', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('SIGLADES')\n",
        "plt.title('Counts of SIGLADES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JArIQTm5MRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are specific cities that have an influence on the delay."
      ],
      "metadata": {
        "id": "oTAeeeyGN3rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='period_day', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('period_day')\n",
        "plt.title('Counts of period_day by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95CeySlAN_ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delays usually occur in the afternoon or nights with the follow ratio Ratio Referring to period day night delay:\n",
        "0.20002305741295826\n",
        "Ratio referring to period day afternoon delay:\n",
        "0.19940406178938289\n",
        "Ratio referring to period day morning delay:\n",
        "0.16007258096327562"
      ],
      "metadata": {
        "id": "mOq7abynOLJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='MES', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('MES')\n",
        "plt.title('Counts of MES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hVxgRU5O4Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the month, it does provide information regarding the delay and it can be assimilated as that in holidays dates (December or July) there are considerable delays."
      ],
      "metadata": {
        "id": "dMJrOvdZRRCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='DIA', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('DIA')\n",
        "plt.title('Counts of MES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLAXaPCnPLt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the day, it does not provide much information and there is an uniform distribution."
      ],
      "metadata": {
        "id": "piBH1lLMQwra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='DIANOM', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('DIANOM')\n",
        "plt.title('Counts of DIANOM by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dnUMIq9G4D6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting the afluent on saturday is lower than the rest of the days and in sunday looks like there is better chances to have a delay."
      ],
      "metadata": {
        "id": "zAak0ao44sCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='high_season', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('high_season')\n",
        "plt.title('Counts of high_season by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htNWSPDa-4JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratio referring to high season (1) and delay:\n",
        "0.24372384937238495\n",
        "Ratio referring to low season (0) and delay:\n",
        "0.17422732236648233"
      ],
      "metadata": {
        "id": "W_eefIPv_f22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='OPERA', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('Flight Delayed by 15 Minutes or More')\n",
        "plt.ylabel('Airline Name')\n",
        "plt.title('Counts of Airline Names by Flight Delay')\n",
        "\n",
        "# adjust the x-axis\n",
        "plt.xticks(range(len(df['delay_15'].unique())), df['delay_15'].unique())\n",
        "plt.legend(labels=[])\n",
        "\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "z0ANa_H85Cgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much information on other flights mostly representing latam and sky airline."
      ],
      "metadata": {
        "id": "qjjKHmht_wKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='TIPOVUELO', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('Flight Delayed by 15 Minutes or More')\n",
        "plt.ylabel('TIPOVUELO')\n",
        "plt.title('Counts of TIPOVUELO by Flight Delay')\n",
        "\n",
        "# adjust the x-axis\n",
        "plt.xticks(range(len(df['delay_15'].unique())), df['delay_15'].unique())\n",
        "\n",
        "\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "FOG5Ze69AHgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "International flights are more prone to delays. But nothing conclusive."
      ],
      "metadata": {
        "id": "lbR_0925AlTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyoff\n",
        "import seaborn as sns\n",
        "\n",
        "corrmat = df.corr()\n",
        "fig = plt.figure(figsize = (12, 9))\n",
        "sns.heatmap(corrmat, vmax = .8, square = True, annot = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "onMUKOYnK0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.- Predictive model"
      ],
      "metadata": {
        "id": "bYEoBPRUBRhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data"
      ],
      "metadata": {
        "id": "TRoMyhjccqOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df_with_rates.info(verbose=True)"
      ],
      "metadata": {
        "id": "4ZWbLyB8THM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "eDsQTyzxivzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPMc4t2mnphA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns only numericals\n",
        "df_m = df.iloc[:,[10,11,12,18,20,22,24,25,26,27,28,29]]\n",
        "df_m.info()"
      ],
      "metadata": {
        "id": "jyfRE6ffbBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select columns 10,11,12,18,20,22,24,25,26,27,28,29\n",
        "cols_1 = [10,11,12,18,20,22,24,25,26,27,28,29]\n",
        "\n",
        "# Select columns 30 to 131\n",
        "#cols_2 = np.r_[30:132]\n",
        "\n",
        "# Combine both lists of columns\n",
        "selected_cols = cols_1 #+ list(cols_2)\n",
        "\n",
        "# Select the columns from the dataframe using iloc\n",
        "train_df = train_df_with_rates.iloc[:, selected_cols]\n",
        "# Select the columns from the dataframe using iloc\n",
        "test_df = test_df_with_rates.iloc[:, selected_cols]"
      ],
      "metadata": {
        "id": "wDd-Sk4dVOwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_m_train = train_df\n",
        "df_m_test = test_df"
      ],
      "metadata": {
        "id": "NjWi9X6931Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# select numerical columns using iloc and dtypes\n",
        "numerical_cols = df_m_train.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# create MinMaxScaler instance and fit on numerical columns\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_m_train[numerical_cols])\n",
        "\n",
        "# apply transform and add new scaled columns with '_sc' suffix\n",
        "df_m_train = df_m_train.copy()  # create a copy of the original DataFrame\n",
        "df_m_train.loc[:, numerical_cols] = scaler.transform(df_m_train[numerical_cols])  # use loc for boolean indexing\n",
        "\n",
        "# add suffix to column names\n",
        "new_col_names = [col + '_sc' for col in numerical_cols]\n",
        "df_m_train.columns = list(df_m_train.columns[:-len(numerical_cols)]) + new_col_names"
      ],
      "metadata": {
        "id": "IudD4PllzgoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# select numerical columns using iloc and dtypes\n",
        "numerical_cols = df_m_test.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# create MinMaxScaler instance and fit on numerical columns\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_m_test[numerical_cols])\n",
        "\n",
        "# apply transform and add new scaled columns with '_sc' suffix\n",
        "df_m_test = df_m_test.copy()  # create a copy of the original DataFrame\n",
        "df_m_test.loc[:, numerical_cols] = scaler.transform(df_m_test[numerical_cols])  # use loc for boolean indexing\n",
        "\n",
        "# add suffix to column names\n",
        "new_col_names = [col + '_sc' for col in numerical_cols]\n",
        "df_m_test.columns = list(df_m_test.columns[:-len(numerical_cols)]) + new_col_names\n"
      ],
      "metadata": {
        "id": "_o4ImqNic08j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Separate the data into X (input features) and y (target variable)\n",
        "\n",
        "X_cols = list(set(df_m_train.columns)-set(['min_diff_sc','delay_15_sc','delay_15_rate_sc']))\n",
        "y_col = ['delay_15_sc']\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "CZ8Y66f1R6EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differents models"
      ],
      "metadata": {
        "id": "FAWUwaJScjcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n"
      ],
      "metadata": {
        "id": "2DiK46nPf_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = LinearRegression(fit_intercept=False, copy_X=False, n_jobs=-1, positive=True)\n",
        "\n",
        "# Create the parameter grid\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "# Perform grid search cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "# Use the best model to make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "ln_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "ln_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "ln_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "ln_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "print('Best params:', grid_search.best_params_)\n",
        "print('Best score:', -grid_search.best_score_)"
      ],
      "metadata": {
        "id": "9aajvP5vgJwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Define the decision tree regressor model\n",
        "dt = DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=None, min_samples_split=2, \n",
        "                           min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
        "                           max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "dt_best = grid_search.best_estimator_\n",
        "\n",
        "# Use the best model to make predictions\n",
        "y_pred = dt_best.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "dt_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "dt_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "dt_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "dt_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "print('Best params:', grid_search.best_params_)\n",
        "print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Root Mean Squared Error for Linear Regression: 0.06390763348078912\n",
        "#Variance score: 0.55\n",
        "#Mape: 544094622945.30\n",
        "#max_error: 0.74\n",
        "#Best params: {'criterion': 'friedman_mse', 'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 6, 'splitter': 'random'}\n",
        "#Best score: -0.5433921721352531"
      ],
      "metadata": {
        "id": "k0Pc8WlXiIUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.neighbors import KNeighborsRegressor\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "#model = KNeighborsRegressor()\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'n_neighbors': [100, 200, 300],\n",
        "#    'weights': ['uniform', 'distance'],\n",
        "#    'metric': ['euclidean', 'manhattan', 'chebyshev', 'canberra'],\n",
        "#    'leaf_size': [20, 30, 40]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Use the best hyperparameters to create the model\n",
        "#best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "#y_pred = best_model.predict(X_test)\n",
        "#print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "#kn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "#print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "#kn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "#print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "#kn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "#print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "#kn_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Root Mean Squared Error for Linear Regression: 0.063890948041988\n",
        "#Variance score: 0.55\n",
        "#Mape: 620469750833.88\n",
        "#max_error: 0.71\n",
        "#Best params: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 100, 'weights': 'uniform'}\n",
        "#Best score: -0.5381708945834038"
      ],
      "metadata": {
        "id": "py5XzieJRAdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model with the best hyperparameters\n",
        "best_model = KNeighborsRegressor(n_neighbors=100, weights='uniform', metric='manhattan', leaf_size=40)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "y_pred = best_model.predict(X_test)\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "kn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "kn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "kn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "kn_e4= (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mLatP9iHCD9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import xgboost as xgb\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, max_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=40, max_depth= 500, \n",
        "#                             eta = 0.2, min_child_weight=1, alpha= 5, num_parallel_tree= 5)\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'max_depth': [3, 5, 7, 10],\n",
        "#    'min_child_weight': [1, 3, 5],\n",
        "#    'subsample': [0.6, 0.8, 1.0],\n",
        "#    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "#    'learning_rate': [0.1, 0.01, 0.001]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "#best_xgb_model = grid_search.best_estimator_\n",
        "#y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Compute the evaluation metrics for the best model\n",
        "#print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "#xgb_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "#print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "#xgb_e2= (metrics.r2_score(y_test, y_pred))\n",
        "#print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "#xgb_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "#print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "#xgb_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0}\n",
        "#Root Mean Squared Error for Linear Regression: 0.0624065238448978\n",
        "#Variance score: 0.57\n",
        "#Mape: 562123968452.08\n",
        "#max_error: 0.73\n",
        "#Best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0}\n",
        "#Best score: -0.5531640844928655"
      ],
      "metadata": {
        "id": "gx0bGcLwmc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, max_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=40, max_depth=7, min_child_weight=1, \n",
        "                                  subsample=0.8, colsample_bytree=0.6, learning_rate=0.1, num_parallel_tree=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Compute the evaluation metrics for the best model\n",
        "print('Root Mean Squared Error for XGBoost:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "xgb_e1 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (r2_score(y_test, y_pred)))\n",
        "xgb_e2 = r2_score(y_test, y_pred)\n",
        "print('Mape: %.2f' % (mean_absolute_percentage_error(y_test, y_pred)))\n",
        "xgb_e3 = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print('max_error: %.2f' % (max_error(y_test, y_pred)))\n",
        "xgb_e4 = max_error(y_test, y_pred)"
      ],
      "metadata": {
        "id": "AptOA0FlDKXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the feature importances\n",
        "xgb.plot_importance(best_xgb_model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2ZMmLC3jyp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "nn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "nn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "nn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "nn_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "og_YK9fFWtZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define a function to create the model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "# Create the model using the best hyperparameters\n",
        "model = create_model()\n",
        "\n",
        "# Train the model on the full training set\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "nn2_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "nn2_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "nn2_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "nn2_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "mfF22uLolVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code used to find the params for nn model."
      ],
      "metadata": {
        "id": "oMzal989-7Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to create the model\n",
        "#def create_model(activation='relu', neurons=64, dropout_rate=0.5):\n",
        "#    model = Sequential()\n",
        "#    model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
        "#    model.add(Dropout(dropout_rate))\n",
        "#    model.add(Dense(neurons//2, activation=activation))\n",
        "#    model.add(Dropout(dropout_rate))\n",
        "#    model.add(Dense(1))\n",
        "#    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "#    return model\n",
        "\n",
        "# Create a KerasRegressor object for use in GridSearchCV\n",
        "#model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'activation': ['relu', 'tanh', 'sigmoid'],\n",
        "#    'neurons': [32, 64, 128],\n",
        "#    'dropout_rate': [0.2, 0.5, 0.8]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)"
      ],
      "metadata": {
        "id": "tYDDdhquXUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code used to find the params for svr model"
      ],
      "metadata": {
        "id": "oWrlChJE_Bkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.svm import SVR\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "#param_grid = {\n",
        "#    'C': [0.1, 1, 10, 100],\n",
        "#    'gamma': [0.01, 0.1, 1, 'scale'],\n",
        "#    'kernel': ['linear', 'rbf', 'poly']\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#svr = SVR()\n",
        "#grid_search = GridSearchCV(svr, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Best params: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
        "#Best score: -0.5193479380405777"
      ],
      "metadata": {
        "id": "d_A1vaZ68uBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Train the SVR model with the best hyperparameters\n",
        "svr = SVR(kernel='rbf', C=10, gamma=0.01)\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "svr_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "svr_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "svr_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "svr_e4= (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "pySz09NzkgkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stack model"
      ],
      "metadata": {
        "id": "4kE1mxUsVI8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we going to analize differents metrics of every model."
      ],
      "metadata": {
        "id": "uHjkfPM53nJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the error arrays\n",
        "dt_error = [dt_e1, dt_e2, dt_e3, dt_e4]\n",
        "xgb_error = [xgb_e1, xgb_e2, xgb_e3, xgb_e4]\n",
        "nn_error = [nn_e1, nn_e2, nn_e3, nn_e4]\n",
        "ln_error = [ln_e1, ln_e2, ln_e3, ln_e4]\n",
        "nn2_error = [nn2_e1, nn2_e2, nn2_e3, nn2_e4]\n",
        "svr_error = [svr_e1, svr_e2, svr_e3, svr_e4]\n",
        "\n",
        "# Calculate the mean of each error metric\n",
        "rmse = [np.mean(dt_error), np.mean(xgb_error), np.mean(nn_error), np.mean(ln_error), np.mean(nn2_error), np.mean(svr_error)]\n",
        "variance_score = [np.mean(dt_e2), np.mean(xgb_e2), np.mean(nn_e2), np.mean(ln_e2), np.mean(nn2_e2), np.mean(svr_e2)]\n",
        "mape = [np.mean(dt_e3), np.mean(xgb_e3), np.mean(nn_e3), np.mean(ln_e3), np.mean(nn2_e3), np.mean(svr_e3)]\n",
        "max_error = [np.mean(dt_e4), np.mean(xgb_e4), np.mean(nn_e4), np.mean(ln_e4), np.mean(nn2_e4), np.mean(svr_e4)]\n",
        "\n",
        "# Plot the error metrics for each model\n",
        "labels = ['Decision Tree', 'XGBoost', 'Neural Network', 'Linear Regression', 'Neural Network 2', 'SVR']\n",
        "\n",
        "# RMSE and MAPE plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, rmse, width, label='RMSE')\n",
        "plt.bar(x + width/2, mape, width, label='MAPE')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('RMSE and MAPE Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Variance score and Max Error plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, r2, width, label='Variance score')\n",
        "plt.bar(x + width/2, max_error, width, label='Max Error')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('Variance score and Max Error Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the mean of each metric\n",
        "print('Mean RMSE:', np.mean(rmse))\n",
        "print('Mean Variance Score:', np.mean(variance_score))\n",
        "print('Mean MAPE:', np.mean(mape))\n",
        "print('Mean Max Error:', np.mean(max_error))"
      ],
      "metadata": {
        "id": "osaFvI4Q20oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Initialize the base models\n",
        "estimators = [('xgb', XGBRegressor(colsample_bytree=1.0, learning_rate=0.1, max_depth=10, min_child_weight=1, subsample=1.0)),\n",
        "              ('svr', SVR(C=1, gamma=0.1, kernel='rbf')),\n",
        "              ('dt', DecisionTreeRegressor(criterion='friedman_mse', max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=6, splitter='random'))]\n",
        "\n",
        "# Initialize the stacking model\n",
        "stack_model = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor())\n",
        "\n",
        "# Train the stacking model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "stack_e1 = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "stack_e2 = (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "stack_e3 = (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "stack_e4 = (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "_i0VKJJ6MGsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to find the best params via gridsearch and using xgb model to ensamble.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qF9iTBjP_ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize the base models\n",
        "estimators = [('xgb', XGBRegressor(colsample_bytree=1.0, learning_rate=0.1, max_depth=10, min_child_weight=1, subsample=1.0)),\n",
        "              ('svr', SVR(C=1, gamma=0.1, kernel='rbf')),\n",
        "              ('dt', DecisionTreeRegressor(criterion='friedman_mse', max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=6, splitter='random'))]\n",
        "\n",
        "# Set the parameter grid for XGBoost\n",
        "xgb_params = {'xgb__learning_rate': [0.01, 0.1, 0.5],\n",
        "              'xgb__max_depth': [3, 5, 10],\n",
        "              'xgb__subsample': [0.5, 0.8, 1.0],\n",
        "              'xgb__colsample_bytree': [0.5, 0.8, 1.0]}\n",
        "\n",
        "# Initialize the stacking model\n",
        "stack_model = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor())\n",
        "\n",
        "# Initialize the grid search object\n",
        "grid_search = GridSearchCV(stack_model, param_grid={'xgb__learning_rate': [0.01, 0.1, 0.5],\n",
        "                                                    'xgb__max_depth': [3, 5, 10],\n",
        "                                                    'xgb__subsample': [0.5, 0.8, 1.0],\n",
        "                                                    'xgb__colsample_bytree': [0.5, 0.8, 1.0]}, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Train the stacking model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "stack_e1 = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "stack_e2 = (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "stack_e3 = (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "stack_e4 = (metrics.max_error(y_test, y_pred)) \n",
        "\n"
      ],
      "metadata": {
        "id": "CNS8Av0iMSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the error arrays\n",
        "dt_error = [dt_e1, dt_e2, dt_e3, dt_e4]\n",
        "xgb_error = [xgb_e1, xgb_e2, xgb_e3, xgb_e4]\n",
        "svr_error = [svr_e1, svr_e2, svr_e3, svr_e4]\n",
        "stack_error = [stack_e1, stack_e2, stack_e3, stack_e4]\n",
        "\n",
        "# Calculate the mean of each error metric\n",
        "rmse = [np.mean(dt_error), np.mean(xgb_error), np.mean(svr_error), np.mean(stack_error)]\n",
        "variance_score = [np.mean(dt_e2), np.mean(xgb_e2), np.mean(svr_e2), np.mean(stack_e2)]\n",
        "mape = [np.mean(dt_e3), np.mean(xgb_e3), np.mean(svr_e3), np.mean(stack_e3)]\n",
        "max_error = [np.mean(dt_e4), np.mean(xgb_e4), np.mean(svr_e4), np.mean(stack_e4)]\n",
        "\n",
        "# Plot the error metrics for each model\n",
        "labels = ['Decision Tree', 'XGBoost', 'SVR', 'Stacking']\n",
        "\n",
        "# RMSE and MAPE plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, rmse, width, label='RMSE')\n",
        "plt.bar(x + width/2, mape, width, label='MAPE')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('RMSE and MAPE Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# variance score and Max Error plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, variance_score, width, label='Variance score')\n",
        "plt.bar(x + width/2, max_error, width, label='Max Error')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('Variance score and Max Error Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the mean of each metric\n",
        "print('Mean RMSE:', np.mean(rmse))\n",
        "print('Mean Variance Score:', np.mean(variance_score))\n",
        "print('Mean MAPE:', np.mean(mape))\n",
        "print('Mean Max Error:', np.mean(max_error))\n",
        "\n"
      ],
      "metadata": {
        "id": "MCbLghdIN0mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.- Results"
      ],
      "metadata": {
        "id": "5EqkSZ2-miHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.- Comments"
      ],
      "metadata": {
        "id": "zk9qftXOmq4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "recommendations to improve the model for predicting delays in airplane takeoff and landing:\n",
        "\n",
        "Add weather information: Weather can have a significant impact on flight delays, so incorporating weather data into the model can help improve its accuracy. Factors such as temperature, precipitation, wind speed, and cloud cover can all affect flight operations.\n",
        "\n",
        "Include additional variables: Adding variables such as gate location, flight distance, online boarding, baggage handling, inflight wifi service, satisfaction, customer type, and gender can provide more information about the factors that contribute to flight delays. This can help the model identify patterns and relationships that may not be captured by the existing variables.\n",
        "\n",
        "Manage outliers: Outliers can have a significant impact on model performance, so it is important to identify and manage them appropriately. This can involve removing outliers or transforming the data to reduce their impact.\n",
        "\n",
        "Use more data: Using more data beyond just the 2017 dataset can provide a more comprehensive understanding of the problem and help the model identify more patterns and relationships. This can lead to better accuracy and generalization of the model.\n",
        "\n",
        "Optimize execution time: As the size of the dataset and complexity of the model increases, it may become necessary to optimize the execution time of the model. This can involve techniques such as parallel processing, distributed computing, or using more efficient algorithms.\n",
        "\n",
        "Overall, these recommendations can help improve the accuracy and generalization of the model for predicting delays in airplane takeoff and landing. By incorporating more variables, managing outliers, using more data, and optimizing execution time, the model can better capture the factors that contribute to flight delays and provide more accurate predictions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "97SEO4LN5PHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper \"Flight delay prediction based on deep learning and Levenberg-Marquart algorithm\", the Levenberg-Marquardt algorithm is used as an optimization algorithm for training the neural network. Specifically, the algorithm is used to minimize the mean squared error between the predicted and actual flight delay values.\n",
        "\n",
        "The Levenberg-Marquardt algorithm is a commonly used optimization algorithm for training neural networks. It combines the steepest descent method with a trust region approach to achieve fast convergence to a local minimum of the objective function. It is particularly well-suited for solving nonlinear least squares problems, which makes it a popular choice for training neural networks."
      ],
      "metadata": {
        "id": "PspfpXpyG-3Y"
      }
    }
  ]
}