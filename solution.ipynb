{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ITdyMoBlcTIAe4O9N78nPgGRB42MafoA",
      "authorship_tag": "ABX9TyOF80yuOQycCzRRejd6rivm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alan-Hans/Challenge-Data-Scientist/blob/developement/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge requires to predict the probability of delay of flights at Santiago de Chile Airport (SCL) using a provided dataset. This work was developed by Alan Hans Bitterlich Koning, email: Alan.bitterlich.k@gmail.com. The index of this notebook is the following: \n",
        "```\n",
        "1.- Import data \n",
        "2.- EDA\n",
        "3.- Models\n",
        "4.- Results\n",
        "5.- Comments\n"
      ],
      "metadata": {
        "id": "YfjHgLyNgSb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.- Import data"
      ],
      "metadata": {
        "id": "nbOzO4KNh3W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Alan-Hans/Challenge-Data-Scientist/main/dataset_SCL.csv'\n",
        "\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZj54iCJgF3J",
        "outputId": "d18bd130-e5f1-486b-eadc-33ecb6ff61d8"
      },
      "execution_count": 657,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-657-b0cb3f0119c9>:9: DtypeWarning:\n",
            "\n",
            "Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "3PvBRIavyPl7",
        "outputId": "ab79c176-1426-49f9-ca47-7fb3980f13a7"
      },
      "execution_count": 658,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
              "0      2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   \n",
              "1      2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   \n",
              "2      2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   \n",
              "3      2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   \n",
              "4      2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   \n",
              "...                    ...   ...   ...   ...   ...                  ...   \n",
              "68201  2017-12-22 14:55:00   400  SCEL  SPJC   JAT  2017-12-22 15:41:00   \n",
              "68202  2017-12-25 14:55:00   400  SCEL  SPJC   JAT  2017-12-25 15:11:00   \n",
              "68203  2017-12-27 14:55:00   400  SCEL  SPJC   JAT  2017-12-27 15:35:00   \n",
              "68204  2017-12-29 14:55:00   400  SCEL  SPJC   JAT  2017-12-29 15:08:00   \n",
              "68205  2017-12-31 14:55:00   400  SCEL  SPJC   JAT  2017-12-31 15:04:00   \n",
              "\n",
              "       Vlo-O Ori-O Des-O Emp-O  DIA  MES   AÑO     DIANOM TIPOVUELO  \\\n",
              "0        226  SCEL  KMIA   AAL    1    1  2017    Domingo         I   \n",
              "1        226  SCEL  KMIA   AAL    2    1  2017      Lunes         I   \n",
              "2        226  SCEL  KMIA   AAL    3    1  2017     Martes         I   \n",
              "3        226  SCEL  KMIA   AAL    4    1  2017  Miercoles         I   \n",
              "4        226  SCEL  KMIA   AAL    5    1  2017     Jueves         I   \n",
              "...      ...   ...   ...   ...  ...  ...   ...        ...       ...   \n",
              "68201  400.0  SCEL  SPJC   JAT   22   12  2017    Viernes         I   \n",
              "68202  400.0  SCEL  SPJC   JAT   25   12  2017      Lunes         I   \n",
              "68203  400.0  SCEL  SPJC   JAT   27   12  2017  Miercoles         I   \n",
              "68204  400.0  SCEL  SPJC   JAT   29   12  2017    Viernes         I   \n",
              "68205  400.0  SCEL  SPJC   JAT   31   12  2017    Domingo         I   \n",
              "\n",
              "                   OPERA  SIGLAORI SIGLADES  \n",
              "0      American Airlines  Santiago    Miami  \n",
              "1      American Airlines  Santiago    Miami  \n",
              "2      American Airlines  Santiago    Miami  \n",
              "3      American Airlines  Santiago    Miami  \n",
              "4      American Airlines  Santiago    Miami  \n",
              "...                  ...       ...      ...  \n",
              "68201       JetSmart SPA  Santiago     Lima  \n",
              "68202       JetSmart SPA  Santiago     Lima  \n",
              "68203       JetSmart SPA  Santiago     Lima  \n",
              "68204       JetSmart SPA  Santiago     Lima  \n",
              "68205       JetSmart SPA  Santiago     Lima  \n",
              "\n",
              "[68206 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ee0a3a0-e829-4b0c-a072-a12dc8017592\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha-I</th>\n",
              "      <th>Vlo-I</th>\n",
              "      <th>Ori-I</th>\n",
              "      <th>Des-I</th>\n",
              "      <th>Emp-I</th>\n",
              "      <th>Fecha-O</th>\n",
              "      <th>Vlo-O</th>\n",
              "      <th>Ori-O</th>\n",
              "      <th>Des-O</th>\n",
              "      <th>Emp-O</th>\n",
              "      <th>DIA</th>\n",
              "      <th>MES</th>\n",
              "      <th>AÑO</th>\n",
              "      <th>DIANOM</th>\n",
              "      <th>TIPOVUELO</th>\n",
              "      <th>OPERA</th>\n",
              "      <th>SIGLAORI</th>\n",
              "      <th>SIGLADES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Lunes</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Martes</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Miercoles</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>Jueves</td>\n",
              "      <td>I</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68201</th>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Viernes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68202</th>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>25</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Lunes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68203</th>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>27</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Miercoles</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68204</th>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Viernes</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68205</th>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "      <td>2017</td>\n",
              "      <td>Domingo</td>\n",
              "      <td>I</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68206 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ee0a3a0-e829-4b0c-a072-a12dc8017592')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ee0a3a0-e829-4b0c-a072-a12dc8017592 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ee0a3a0-e829-4b0c-a072-a12dc8017592');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 658
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of every column\n",
        "```\n",
        "Fecha-I: Scheduled date and time of departure (local time) in format \"YYYY-MM-DD hh:mm:ss\" (year-month-day hour:minute:second).\n",
        "Vlo-I: Scheduled flight number (text)\n",
        "Ori-I: Origin city code (text)\n",
        "Des-I: destination city code. (text)\n",
        "Emp-I: Scheduled airline code (text)\n",
        "Fecha-O: Date and time of arrival (local time) in format \"YYYY-MM-DD hh:mm:ss\" (year-month-day hour:minute:second).\n",
        "Vlo-O: Flight operation number of the flight (text)\n",
        "Ori-O: Operation origin city code (text)\n",
        "Des-O: Operation destination city code (text)\n",
        "Emp-O: Airline code of the operated flight (text)\n",
        "DIA: Day of the month (numeric)\n",
        "MES: Month of the year (numeric)\n",
        "AÑO: Year (numeric)\n",
        "DIANOM: Day of the week (text)\n",
        "TIPOVUELO: Type of flight, I =International, N =National (text)\n",
        "OPERA: Operating airline company (text)\n",
        "SIGLAORI: Name city of origin (text)\n",
        "SIGLADES: Destination city name (text)'\n",
        "\n"
      ],
      "metadata": {
        "id": "GBBrqwvr5Vuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.- EDA"
      ],
      "metadata": {
        "id": "dDOyioTkyZEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "#Some interesting insights from this analysis: \n",
        "\n",
        "*   It shows that the largest number of flights in this data correspond to Grupo LATAM airlines with about 58%.\n",
        "*   The Top 5 destinations are: Buenos Aires,     Antofagasta,   Lima,             \n",
        "Calama and           Puerto Montt.\n",
        "*   Most flights are made during the month of December and on Fridays.\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "Du4GcvcDDTnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print the first few rows of the dataset to get a sense of its structure\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(df.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Descriptive statistics\n",
        "print(df.describe())\n",
        "\n",
        "# List of columns\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtVdCHLMzTDa",
        "outputId": "1388cbb3-9881-4a85-d4a3-2ff1fe66b138"
      },
      "execution_count": 659,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O Vlo-O  \\\n",
            "0  2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   226   \n",
            "1  2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   226   \n",
            "2  2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   226   \n",
            "3  2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   226   \n",
            "4  2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   226   \n",
            "\n",
            "  Ori-O Des-O Emp-O  DIA  MES   AÑO     DIANOM TIPOVUELO              OPERA  \\\n",
            "0  SCEL  KMIA   AAL    1    1  2017    Domingo         I  American Airlines   \n",
            "1  SCEL  KMIA   AAL    2    1  2017      Lunes         I  American Airlines   \n",
            "2  SCEL  KMIA   AAL    3    1  2017     Martes         I  American Airlines   \n",
            "3  SCEL  KMIA   AAL    4    1  2017  Miercoles         I  American Airlines   \n",
            "4  SCEL  KMIA   AAL    5    1  2017     Jueves         I  American Airlines   \n",
            "\n",
            "   SIGLAORI SIGLADES  \n",
            "0  Santiago    Miami  \n",
            "1  Santiago    Miami  \n",
            "2  Santiago    Miami  \n",
            "3  Santiago    Miami  \n",
            "4  Santiago    Miami  \n",
            "(68206, 18)\n",
            "Fecha-I      0\n",
            "Vlo-I        0\n",
            "Ori-I        0\n",
            "Des-I        0\n",
            "Emp-I        0\n",
            "Fecha-O      0\n",
            "Vlo-O        1\n",
            "Ori-O        0\n",
            "Des-O        0\n",
            "Emp-O        0\n",
            "DIA          0\n",
            "MES          0\n",
            "AÑO          0\n",
            "DIANOM       0\n",
            "TIPOVUELO    0\n",
            "OPERA        0\n",
            "SIGLAORI     0\n",
            "SIGLADES     0\n",
            "dtype: int64\n",
            "                DIA           MES           AÑO\n",
            "count  68206.000000  68206.000000  68206.000000\n",
            "mean      15.714790      6.622585   2017.000029\n",
            "std        8.782886      3.523321      0.005415\n",
            "min        1.000000      1.000000   2017.000000\n",
            "25%        8.000000      3.000000   2017.000000\n",
            "50%       16.000000      7.000000   2017.000000\n",
            "75%       23.000000     10.000000   2017.000000\n",
            "max       31.000000     12.000000   2018.000000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 68206 entries, 0 to 68205\n",
            "Data columns (total 18 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Fecha-I    68206 non-null  object\n",
            " 1   Vlo-I      68206 non-null  object\n",
            " 2   Ori-I      68206 non-null  object\n",
            " 3   Des-I      68206 non-null  object\n",
            " 4   Emp-I      68206 non-null  object\n",
            " 5   Fecha-O    68206 non-null  object\n",
            " 6   Vlo-O      68205 non-null  object\n",
            " 7   Ori-O      68206 non-null  object\n",
            " 8   Des-O      68206 non-null  object\n",
            " 9   Emp-O      68206 non-null  object\n",
            " 10  DIA        68206 non-null  int64 \n",
            " 11  MES        68206 non-null  int64 \n",
            " 12  AÑO        68206 non-null  int64 \n",
            " 13  DIANOM     68206 non-null  object\n",
            " 14  TIPOVUELO  68206 non-null  object\n",
            " 15  OPERA      68206 non-null  object\n",
            " 16  SIGLAORI   68206 non-null  object\n",
            " 17  SIGLADES   68206 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 9.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## For the purpose of this analysis, null results will be filtered out, which apparently corresponds to only 1, so it will not have a great impact and drop duplicate if there are.\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xlSBRBj1YwL",
        "outputId": "47785380-971a-49a7-8e78-ef511be5962b"
      },
      "execution_count": 660,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 18 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Fecha-I    68205 non-null  object\n",
            " 1   Vlo-I      68205 non-null  object\n",
            " 2   Ori-I      68205 non-null  object\n",
            " 3   Des-I      68205 non-null  object\n",
            " 4   Emp-I      68205 non-null  object\n",
            " 5   Fecha-O    68205 non-null  object\n",
            " 6   Vlo-O      68205 non-null  object\n",
            " 7   Ori-O      68205 non-null  object\n",
            " 8   Des-O      68205 non-null  object\n",
            " 9   Emp-O      68205 non-null  object\n",
            " 10  DIA        68205 non-null  int64 \n",
            " 11  MES        68205 non-null  int64 \n",
            " 12  AÑO        68205 non-null  int64 \n",
            " 13  DIANOM     68205 non-null  object\n",
            " 14  TIPOVUELO  68205 non-null  object\n",
            " 15  OPERA      68205 non-null  object\n",
            " 16  SIGLAORI   68205 non-null  object\n",
            " 17  SIGLADES   68205 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 9.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "    print(i)\n",
        "    print('Unique Values: '+str(len(df.groupby([i]).count())))\n",
        "    print(df[i].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01dRNfeoAsWJ",
        "outputId": "492c6dbe-764c-45e5-d5f7-3b791db44ee5"
      },
      "execution_count": 661,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fecha-I\n",
            "Unique Values: 53252\n",
            "2017-07-28 13:30:00    6\n",
            "2017-03-19 18:00:00    6\n",
            "2017-03-26 18:00:00    6\n",
            "2017-12-16 13:35:00    5\n",
            "2017-02-15 13:35:00    5\n",
            "                      ..\n",
            "2017-06-01 15:16:00    1\n",
            "2017-06-02 15:16:00    1\n",
            "2017-06-03 15:16:00    1\n",
            "2017-06-04 15:16:00    1\n",
            "2017-12-31 14:55:00    1\n",
            "Name: Fecha-I, Length: 53252, dtype: int64\n",
            "Vlo-I\n",
            "Unique Values: 750\n",
            "174     686\n",
            "11      645\n",
            "116     608\n",
            "150     557\n",
            "162     553\n",
            "       ... \n",
            "1158      1\n",
            "368       1\n",
            "9955      1\n",
            "9701      1\n",
            "1218      1\n",
            "Name: Vlo-I, Length: 750, dtype: int64\n",
            "Ori-I\n",
            "Unique Values: 1\n",
            "SCEL    68205\n",
            "Name: Ori-I, dtype: int64\n",
            "Des-I\n",
            "Unique Values: 64\n",
            "SCFA    5787\n",
            "SPJC    5269\n",
            "SCCF    5145\n",
            "SCTE    4357\n",
            "SCIE    3995\n",
            "        ... \n",
            "SBFI       1\n",
            "SPSO       1\n",
            "SEQU       1\n",
            "SEQM       1\n",
            "SARI       1\n",
            "Name: Des-I, Length: 64, dtype: int64\n",
            "Emp-I\n",
            "Unique Values: 30\n",
            "LAN    37611\n",
            "SKU    14298\n",
            "TAM     3049\n",
            "ARG     1949\n",
            "CMP     1850\n",
            "LAW     1573\n",
            "AVA     1152\n",
            "JAT     1095\n",
            "GLO      806\n",
            "AAL      757\n",
            "ACA      565\n",
            "IBE      362\n",
            "AFR      358\n",
            "DAL      358\n",
            "AMX      351\n",
            "UAL      335\n",
            "ONE      279\n",
            "AZA      259\n",
            "KLM      251\n",
            "LAP      216\n",
            "BAW      205\n",
            "QFU      195\n",
            "JMR      100\n",
            "LRC       92\n",
            "AUT       74\n",
            "PUE       49\n",
            "LXP        9\n",
            "LPE        4\n",
            "DSM        2\n",
            "LNE        1\n",
            "Name: Emp-I, dtype: int64\n",
            "Fecha-O\n",
            "Unique Values: 62774\n",
            "2017-05-19 07:01:00    5\n",
            "2017-11-05 14:51:00    5\n",
            "2017-10-25 07:37:00    4\n",
            "2017-03-08 13:34:00    4\n",
            "2017-09-07 20:10:00    4\n",
            "                      ..\n",
            "2017-05-04 15:48:00    1\n",
            "2017-05-05 17:02:00    1\n",
            "2017-05-07 17:03:00    1\n",
            "2017-05-08 17:06:00    1\n",
            "2017-12-31 15:04:00    1\n",
            "Name: Fecha-O, Length: 62774, dtype: int64\n",
            "Vlo-O\n",
            "Unique Values: 866\n",
            "174      649\n",
            "11       646\n",
            "116      608\n",
            "150      517\n",
            "704      514\n",
            "        ... \n",
            "356        1\n",
            "1148       1\n",
            "846A       1\n",
            "4950       1\n",
            "180.0      1\n",
            "Name: Vlo-O, Length: 866, dtype: int64\n",
            "Ori-O\n",
            "Unique Values: 1\n",
            "SCEL    68205\n",
            "Name: Ori-O, dtype: int64\n",
            "Des-O\n",
            "Unique Values: 63\n",
            "SCFA    5786\n",
            "SPJC    5269\n",
            "SCCF    5146\n",
            "SCTE    4357\n",
            "SCIE    3993\n",
            "        ... \n",
            "SEQM       2\n",
            "KIAD       1\n",
            "SPSO       1\n",
            "EGYP       1\n",
            "SLCB       1\n",
            "Name: Des-O, Length: 63, dtype: int64\n",
            "Emp-O\n",
            "Unique Values: 32\n",
            "LAN    20988\n",
            "LXP    14558\n",
            "SKU    14298\n",
            "TAM     3046\n",
            "ARG     1946\n",
            "CMP     1850\n",
            "JMR     1647\n",
            "LPE     1214\n",
            "JAT     1095\n",
            "AVA      885\n",
            "GLO      806\n",
            "AAL      757\n",
            "ACA      565\n",
            "DSM      493\n",
            "LNE      374\n",
            "IBE      362\n",
            "AFR      358\n",
            "DAL      358\n",
            "LRC      357\n",
            "AMX      351\n",
            "UAL      335\n",
            "ONE      279\n",
            "AZA      259\n",
            "KLM      251\n",
            "LAP      219\n",
            "BAW      205\n",
            "QFA      195\n",
            "AUT       77\n",
            "PUE       49\n",
            "56R       16\n",
            "48O       10\n",
            "TPU        2\n",
            "Name: Emp-O, dtype: int64\n",
            "DIA\n",
            "Unique Values: 31\n",
            "20    2290\n",
            "27    2286\n",
            "12    2284\n",
            "10    2283\n",
            "6     2275\n",
            "22    2272\n",
            "13    2272\n",
            "3     2271\n",
            "16    2268\n",
            "26    2267\n",
            "21    2267\n",
            "7     2264\n",
            "23    2260\n",
            "5     2241\n",
            "15    2239\n",
            "11    2237\n",
            "19    2237\n",
            "24    2232\n",
            "28    2232\n",
            "17    2228\n",
            "9     2227\n",
            "2     2223\n",
            "14    2221\n",
            "4     2215\n",
            "8     2213\n",
            "1     2208\n",
            "25    2179\n",
            "18    2160\n",
            "29    2044\n",
            "30    2020\n",
            "31    1290\n",
            "Name: DIA, dtype: int64\n",
            "MES\n",
            "Unique Values: 12\n",
            "12    6356\n",
            "1     6107\n",
            "11    6080\n",
            "10    6032\n",
            "7     5992\n",
            "8     5744\n",
            "9     5610\n",
            "2     5561\n",
            "3     5482\n",
            "5     5240\n",
            "4     5020\n",
            "6     4981\n",
            "Name: MES, dtype: int64\n",
            "AÑO\n",
            "Unique Values: 2\n",
            "2017    68203\n",
            "2018        2\n",
            "Name: AÑO, dtype: int64\n",
            "DIANOM\n",
            "Unique Values: 7\n",
            "Viernes      10292\n",
            "Jueves       10250\n",
            "Lunes        10131\n",
            "Domingo       9796\n",
            "Miercoles     9722\n",
            "Martes        9662\n",
            "Sabado        8352\n",
            "Name: DIANOM, dtype: int64\n",
            "TIPOVUELO\n",
            "Unique Values: 2\n",
            "N    36966\n",
            "I    31239\n",
            "Name: TIPOVUELO, dtype: int64\n",
            "OPERA\n",
            "Unique Values: 23\n",
            "Grupo LATAM                 40892\n",
            "Sky Airline                 14298\n",
            "Aerolineas Argentinas        1949\n",
            "Copa Air                     1850\n",
            "Latin American Wings         1673\n",
            "Avianca                      1152\n",
            "JetSmart SPA                 1095\n",
            "Gol Trans                     806\n",
            "American Airlines             757\n",
            "Air Canada                    565\n",
            "Iberia                        362\n",
            "Delta Air                     358\n",
            "Air France                    358\n",
            "Aeromexico                    351\n",
            "United Airlines               335\n",
            "Oceanair Linhas Aereas        279\n",
            "Alitalia                      259\n",
            "K.L.M.                        251\n",
            "British Airways               205\n",
            "Qantas Airways                195\n",
            "Lacsa                          92\n",
            "Austral                        74\n",
            "Plus Ultra Lineas Aereas       49\n",
            "Name: OPERA, dtype: int64\n",
            "SIGLAORI\n",
            "Unique Values: 1\n",
            "Santiago    68205\n",
            "Name: SIGLAORI, dtype: int64\n",
            "SIGLADES\n",
            "Unique Values: 62\n",
            "Buenos Aires      6335\n",
            "Antofagasta       5786\n",
            "Lima              5269\n",
            "Calama            5146\n",
            "Puerto Montt      4357\n",
            "                  ... \n",
            "Quito                2\n",
            "Washington           1\n",
            "Pisco, Peru          1\n",
            "Puerto Stanley       1\n",
            "Cochabamba           1\n",
            "Name: SIGLADES, Length: 62, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new categorical column based on Column1\n",
        "df['TIPOVUELO_int'] = df['TIPOVUELO'].apply(lambda x: 1 if x == 'I' else 0)\n",
        "\n",
        "# Print the updated dataframe\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6knD0dim1o",
        "outputId": "bf121be5-94ef-481e-a9a9-f5a024a4bc7d"
      },
      "execution_count": 662,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Fecha-I        68205 non-null  object\n",
            " 1   Vlo-I          68205 non-null  object\n",
            " 2   Ori-I          68205 non-null  object\n",
            " 3   Des-I          68205 non-null  object\n",
            " 4   Emp-I          68205 non-null  object\n",
            " 5   Fecha-O        68205 non-null  object\n",
            " 6   Vlo-O          68205 non-null  object\n",
            " 7   Ori-O          68205 non-null  object\n",
            " 8   Des-O          68205 non-null  object\n",
            " 9   Emp-O          68205 non-null  object\n",
            " 10  DIA            68205 non-null  int64 \n",
            " 11  MES            68205 non-null  int64 \n",
            " 12  AÑO            68205 non-null  int64 \n",
            " 13  DIANOM         68205 non-null  object\n",
            " 14  TIPOVUELO      68205 non-null  object\n",
            " 15  OPERA          68205 non-null  object\n",
            " 16  SIGLAORI       68205 non-null  object\n",
            " 17  SIGLADES       68205 non-null  object\n",
            " 18  TIPOVUELO_int  68205 non-null  int64 \n",
            "dtypes: int64(4), object(15)\n",
            "memory usage: 10.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creation of feature variables"
      ],
      "metadata": {
        "id": "irdDhtNasx3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your date column is called \"Date-I\"\n",
        "# Create a datetime object from the \"Date-I\" column\n",
        "df['Date-I'] = pd.to_datetime(df['Fecha-I'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Define the high season periods\n",
        "high_season_periods = [\n",
        "    (pd.to_datetime('12-15-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('03-03-2018', format=\"%m-%d-%Y\").date()),\n",
        "    (pd.to_datetime('07-15-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('07-31-2017', format=\"%m-%d-%Y\").date()),\n",
        "    (pd.to_datetime('09-11-2017', format=\"%m-%d-%Y\").date(), pd.to_datetime('09-30-2017', format=\"%m-%d-%Y\").date())\n",
        "]\n",
        "\n",
        "# Define a function to check if a date is in a high season period\n",
        "def is_in_high_season(date):\n",
        "    for period in high_season_periods:\n",
        "        if period[0] <= date.date() <= period[1]:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "# Apply the function to create the \"high_season\" column\n",
        "df['high_season'] = df['Date-I'].apply(is_in_high_season)"
      ],
      "metadata": {
        "id": "giUpkya5k7jX"
      },
      "execution_count": 663,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to general instructions, it should be considered landing and takeoff of aircraft but it is not clear according to times if it is landing or taking off, variables are reviewed and it is not possible to conclude if it is landing or taking off without an extra variable. Therefore we proceed to create the function taking into consideration that only time will be subtracted and in case the result is negative it will be consider as negative in order to enter to the predicction."
      ],
      "metadata": {
        "id": "dlL-p3NSPg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Date-I'] = pd.to_datetime(df['Fecha-I'])\n",
        "df['Date-O'] = pd.to_datetime(df['Fecha-O'])\n",
        "\n",
        "# Handle negative values in min_diff column\n",
        "# Calculate the time difference between two columns\n",
        "df['min_diff'] = df.apply(lambda row: (row['Date-O'] - row['Date-I']).total_seconds() / 60 if row['Date-O'] > row['Date-I'] else (row['Date-I'] - row['Date-O']).total_seconds() / -60, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add period_day column based on Date-I\n",
        "df['period_day'] = df['Date-I'].apply(lambda x: 'morning' if 5 <= x.hour < 12 else ('afternoon' if 12 <= x.hour < 19 else 'night'))\n",
        "\n",
        "# Create a function to set delay_15 column based on min_diff\n",
        "def set_delay_15(x):\n",
        "    if x > 15:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Apply the set_delay_15 function to create delay_15 column\n",
        "df['delay_15'] = df['min_diff'].apply(set_delay_15)\n",
        "\n",
        "# Print the updated dataframe\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "861ivnJPtHJp",
        "outputId": "3389a8fd-460b-40dd-cd4c-95a10faf6d77"
      },
      "execution_count": 664,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O  \\\n",
              "0      2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   \n",
              "1      2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   \n",
              "2      2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   \n",
              "3      2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   \n",
              "4      2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   \n",
              "...                    ...   ...   ...   ...   ...                  ...   \n",
              "68201  2017-12-22 14:55:00   400  SCEL  SPJC   JAT  2017-12-22 15:41:00   \n",
              "68202  2017-12-25 14:55:00   400  SCEL  SPJC   JAT  2017-12-25 15:11:00   \n",
              "68203  2017-12-27 14:55:00   400  SCEL  SPJC   JAT  2017-12-27 15:35:00   \n",
              "68204  2017-12-29 14:55:00   400  SCEL  SPJC   JAT  2017-12-29 15:08:00   \n",
              "68205  2017-12-31 14:55:00   400  SCEL  SPJC   JAT  2017-12-31 15:04:00   \n",
              "\n",
              "       Vlo-O Ori-O Des-O Emp-O  ...              OPERA  SIGLAORI  SIGLADES  \\\n",
              "0        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "1        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "2        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "3        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "4        226  SCEL  KMIA   AAL  ...  American Airlines  Santiago     Miami   \n",
              "...      ...   ...   ...   ...  ...                ...       ...       ...   \n",
              "68201  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68202  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68203  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68204  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "68205  400.0  SCEL  SPJC   JAT  ...       JetSmart SPA  Santiago      Lima   \n",
              "\n",
              "      TIPOVUELO_int              Date-I high_season              Date-O  \\\n",
              "0                 1 2017-01-01 23:30:00           0 2017-01-01 23:33:00   \n",
              "1                 1 2017-01-02 23:30:00           0 2017-01-02 23:39:00   \n",
              "2                 1 2017-01-03 23:30:00           0 2017-01-03 23:39:00   \n",
              "3                 1 2017-01-04 23:30:00           0 2017-01-04 23:33:00   \n",
              "4                 1 2017-01-05 23:30:00           0 2017-01-05 23:28:00   \n",
              "...             ...                 ...         ...                 ...   \n",
              "68201             1 2017-12-22 14:55:00           1 2017-12-22 15:41:00   \n",
              "68202             1 2017-12-25 14:55:00           1 2017-12-25 15:11:00   \n",
              "68203             1 2017-12-27 14:55:00           1 2017-12-27 15:35:00   \n",
              "68204             1 2017-12-29 14:55:00           1 2017-12-29 15:08:00   \n",
              "68205             1 2017-12-31 14:55:00           1 2017-12-31 15:04:00   \n",
              "\n",
              "      min_diff  period_day delay_15  \n",
              "0          3.0       night        0  \n",
              "1          9.0       night        0  \n",
              "2          9.0       night        0  \n",
              "3          3.0       night        0  \n",
              "4         -2.0       night        0  \n",
              "...        ...         ...      ...  \n",
              "68201     46.0   afternoon        1  \n",
              "68202     16.0   afternoon        1  \n",
              "68203     40.0   afternoon        1  \n",
              "68204     13.0   afternoon        0  \n",
              "68205      9.0   afternoon        0  \n",
              "\n",
              "[68205 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f063b1bf-3263-4416-b898-38911f50db1d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha-I</th>\n",
              "      <th>Vlo-I</th>\n",
              "      <th>Ori-I</th>\n",
              "      <th>Des-I</th>\n",
              "      <th>Emp-I</th>\n",
              "      <th>Fecha-O</th>\n",
              "      <th>Vlo-O</th>\n",
              "      <th>Ori-O</th>\n",
              "      <th>Des-O</th>\n",
              "      <th>Emp-O</th>\n",
              "      <th>...</th>\n",
              "      <th>OPERA</th>\n",
              "      <th>SIGLAORI</th>\n",
              "      <th>SIGLADES</th>\n",
              "      <th>TIPOVUELO_int</th>\n",
              "      <th>Date-I</th>\n",
              "      <th>high_season</th>\n",
              "      <th>Date-O</th>\n",
              "      <th>min_diff</th>\n",
              "      <th>period_day</th>\n",
              "      <th>delay_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-01 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-01 23:33:00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-02 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-02 23:39:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-03 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-03 23:39:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-04 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-04 23:33:00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>226</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>KMIA</td>\n",
              "      <td>AAL</td>\n",
              "      <td>...</td>\n",
              "      <td>American Airlines</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Miami</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-05 23:30:00</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-05 23:28:00</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68201</th>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-22 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-22 15:41:00</td>\n",
              "      <td>46.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68202</th>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-25 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-25 15:11:00</td>\n",
              "      <td>16.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68203</th>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-27 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-27 15:35:00</td>\n",
              "      <td>40.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68204</th>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-29 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-29 15:08:00</td>\n",
              "      <td>13.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68205</th>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>400</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>400.0</td>\n",
              "      <td>SCEL</td>\n",
              "      <td>SPJC</td>\n",
              "      <td>JAT</td>\n",
              "      <td>...</td>\n",
              "      <td>JetSmart SPA</td>\n",
              "      <td>Santiago</td>\n",
              "      <td>Lima</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-31 14:55:00</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-31 15:04:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68205 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f063b1bf-3263-4416-b898-38911f50db1d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f063b1bf-3263-4416-b898-38911f50db1d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f063b1bf-3263-4416-b898-38911f50db1d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 664
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionary to map string values to integers\n",
        "mapping_dict = {\n",
        "    'Lunes': 0,\n",
        "    'Martes': 1,\n",
        "    'Miercoles': 2,\n",
        "    'Jueves': 3,\n",
        "    'Viernes': 4,\n",
        "    'Sabado': 5,\n",
        "    'Domingo': 6\n",
        "}\n",
        "\n",
        "# create new column using map method with dictionary\n",
        "df['DIANOM_int'] = df['DIANOM'].map(mapping_dict)\n",
        "\n",
        "# add 7 as a possible value (since there are 7 string values)\n",
        "df['DIANOM_int'] = df['DIANOM_int'].fillna(7).astype(int)"
      ],
      "metadata": {
        "id": "0r3MDbuuLpjD"
      },
      "execution_count": 665,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionary to map string values to integers\n",
        "mapping_dict = {\n",
        "    'morning': 0,\n",
        "    'afternoon': 1,\n",
        "    'night': 2,\n",
        "}\n",
        "\n",
        "# create new column using map method with dictionary\n",
        "df['period_day_int'] = df['period_day'].map(mapping_dict)\n",
        "\n",
        "# add 7 as a possible value (since there are 7 string values)\n",
        "df['period_day_int'] = df['period_day_int'].fillna(7).astype(int)"
      ],
      "metadata": {
        "id": "JVKL3ZrLM3O1"
      },
      "execution_count": 666,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var = ['OPERA','SIGLAORI',\"SIGLADES\"]\n",
        "for i in cat_var:\n",
        "   name = i + '_int'\n",
        "   df[name] = pd.factorize(df[i])[0]\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg0vQyCyZtP-",
        "outputId": "5da41c81-2c84-4a5c-b63b-a9c6381b539b"
      },
      "execution_count": 667,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 68205 entries, 0 to 68205\n",
            "Data columns (total 30 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   Fecha-I         68205 non-null  object        \n",
            " 1   Vlo-I           68205 non-null  object        \n",
            " 2   Ori-I           68205 non-null  object        \n",
            " 3   Des-I           68205 non-null  object        \n",
            " 4   Emp-I           68205 non-null  object        \n",
            " 5   Fecha-O         68205 non-null  object        \n",
            " 6   Vlo-O           68205 non-null  object        \n",
            " 7   Ori-O           68205 non-null  object        \n",
            " 8   Des-O           68205 non-null  object        \n",
            " 9   Emp-O           68205 non-null  object        \n",
            " 10  DIA             68205 non-null  int64         \n",
            " 11  MES             68205 non-null  int64         \n",
            " 12  AÑO             68205 non-null  int64         \n",
            " 13  DIANOM          68205 non-null  object        \n",
            " 14  TIPOVUELO       68205 non-null  object        \n",
            " 15  OPERA           68205 non-null  object        \n",
            " 16  SIGLAORI        68205 non-null  object        \n",
            " 17  SIGLADES        68205 non-null  object        \n",
            " 18  TIPOVUELO_int   68205 non-null  int64         \n",
            " 19  Date-I          68205 non-null  datetime64[ns]\n",
            " 20  high_season     68205 non-null  int64         \n",
            " 21  Date-O          68205 non-null  datetime64[ns]\n",
            " 22  min_diff        68205 non-null  float64       \n",
            " 23  period_day      68205 non-null  object        \n",
            " 24  delay_15        68205 non-null  int64         \n",
            " 25  DIANOM_int      68205 non-null  int64         \n",
            " 26  period_day_int  68205 non-null  int64         \n",
            " 27  OPERA_int       68205 non-null  int64         \n",
            " 28  SIGLAORI_int    68205 non-null  int64         \n",
            " 29  SIGLADES_int    68205 non-null  int64         \n",
            "dtypes: datetime64[ns](2), float64(1), int64(11), object(16)\n",
            "memory usage: 16.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library and alias it as \"pd\"\n",
        "import pandas as pd\n",
        "\n",
        "# Select only the columns we want to keep in a new DataFrame called \"df_export\"\n",
        "# Here, we assume the original DataFrame is called \"df\"\n",
        "df_export = df[['high_season', 'min_diff', 'delay_15', 'period_day']]\n",
        "\n",
        "# Export the new DataFrame to a CSV file named \"syntethic_feature.csv\"\n",
        "# The \"index=False\" parameter prevents the row index from being written to the file\n",
        "# The \"header=True\" parameter writes the column names to the file\n",
        "df_export.to_csv(\"syntethic_feature.csv\", index=False, header=True)"
      ],
      "metadata": {
        "id": "gtgwL29Q1dDz"
      },
      "execution_count": 668,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outliers treatment"
      ],
      "metadata": {
        "id": "C5_Y2yg5lBms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(df,features):\n",
        "    outlier_indices=[]\n",
        "    \n",
        "    for c in features:\n",
        "        # 1st quartile\n",
        "        Q1=np.percentile(df[c],25)\n",
        "        \n",
        "        # 3rd quartile\n",
        "        Q3=np.percentile(df[c],75)\n",
        "        \n",
        "        # IQR\n",
        "        IQR= Q3-Q1\n",
        "        \n",
        "        # Outlier Step\n",
        "        outlier_step= IQR * 2\n",
        "        \n",
        "        # Detect outlier and their indeces \n",
        "        outlier_list_col = df[(df[c]< Q1 - outlier_step)|( df[c] > Q3 + outlier_step)].index\n",
        "        \n",
        "        # Store indices \n",
        "        outlier_indices.extend(outlier_list_col)\n",
        "    \n",
        "    outliers_indices = Counter(outlier_indices)\n",
        "    multiple_outliers = list(i for i , v in outliers_indices.items() if v>2 )\n",
        "    return multiple_outliers"
      ],
      "metadata": {
        "id": "cTpYixhwlCyf"
      },
      "execution_count": 669,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from collections import Counter\n",
        "\n",
        "df.loc[detect_outliers(df,['DIA',\n",
        " 'MES',\n",
        " 'AÑO',\n",
        " 'TIPOVUELO_int',\n",
        " 'TIPOVUELO_int',\n",
        " 'Date-I',\n",
        " 'high_season',\n",
        " 'Date-O',\n",
        " 'min_diff',\n",
        " 'delay_15',\n",
        " 'DIANOM_int',\n",
        " 'period_day_int',\n",
        " 'OPERA_int',\n",
        " 'SIGLAORI_int',\n",
        " 'SIGLADES_int'])]"
      ],
      "metadata": {
        "id": "5KTQDDmYlIDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(detect_outliers(df,['DIA',\n",
        " 'MES',\n",
        " 'AÑO',\n",
        " 'TIPOVUELO_int',\n",
        " 'TIPOVUELO_int',\n",
        " 'Date-I',\n",
        " 'high_season',\n",
        " 'Date-O',\n",
        " 'min_diff',\n",
        " 'delay_15',\n",
        " 'DIANOM_int',\n",
        " 'period_day_int',\n",
        " 'OPERA_int',\n",
        " 'SIGLAORI_int',\n",
        " 'SIGLADES_int']),axis = 0).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "ihrBB8TqlRAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create variable of rain"
      ],
      "metadata": {
        "id": "K2A23QwKZ6_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data from rainy 24hours of water from santiago de chile, data extracted from https://climatologia.meteochile.gob.cl/"
      ],
      "metadata": {
        "id": "4b1SbDjbh8Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Alan-Hans/Challenge-Data-Scientist/developement/data_agua24horas.csv\"\n",
        "df_rain = pd.read_csv(url, delimiter=\";\",  header=[0])\n"
      ],
      "metadata": {
        "id": "P3bpZQzVaISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rain"
      ],
      "metadata": {
        "id": "ypUbeKndhdJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKTydUzCivLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# convert the date column to a period object and extract year, month, and day\n",
        "df_rain['period'] = pd.to_datetime(df_rain['date'], format='%d-%m-%y %H:%M').dt.to_period('D')\n",
        "df_rain['period'] = df_rain['period'].dt.year.astype(str).str.zfill(2) + \\\n",
        "                df_rain['period'].dt.month.astype(str).str.zfill(2) + \\\n",
        "                df_rain['period'].dt.day.astype(str).str.zfill(2)\n",
        "\n",
        "# display the dataframe\n",
        "print(df_rain)\n",
        "\n"
      ],
      "metadata": {
        "id": "AyClCTybg3NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate to the completly data."
      ],
      "metadata": {
        "id": "ScUszWV0ixLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# convert the date column to a period object and extract year, month, and day\n",
        "df['period'] = pd.to_datetime(df['Fecha-I'], format='%Y-%m-%d %H:%M:%S').dt.to_period('D')\n",
        "df['period'] = df['period'].dt.year.astype(str).str.zfill(2) + \\\n",
        "                df['period'].dt.month.astype(str).str.zfill(2) + \\\n",
        "                df['period'].dt.day.astype(str).str.zfill(2)\n",
        "\n",
        "# display the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "D4Klw_zmiwOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df, df_rain, on='period')\n"
      ],
      "metadata": {
        "id": "kB1VF575j94y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = merged_df"
      ],
      "metadata": {
        "id": "ON_A8g3dkEXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create probability variables"
      ],
      "metadata": {
        "id": "hsV3bDXoBQra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we add the probability to happen to have a delay_15 depending on OPERA', 'SIGLAORI', 'SIGLADES', in order to add the impact of this variables to the predicction, according to our analisis SIGLAORI, do not give information."
      ],
      "metadata": {
        "id": "UpYH2koU_T62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "CPKXgvlL-KAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# select the relevant columns for the analysis\n",
        "df_selected = df[[ 'OPERA', 'SIGLAORI', 'SIGLADES']]\n",
        "\n",
        "# convert categorical variables into binary variables\n",
        "df_binary = pd.get_dummies(df, columns=[ 'OPERA', 'SIGLAORI', 'SIGLADES'], drop_first=True)\n",
        "\n",
        "# concatenate the original DataFrame and the binary DataFrame\n",
        "df_with_dummies = pd.concat([df, df_binary], axis=1)\n",
        "\n",
        "# print the resulting DataFrame\n",
        "print(df_with_dummies)"
      ],
      "metadata": {
        "id": "cezFdE9OnxB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the column names of the dummy variables\n",
        "dummy_cols = list(df_binary.columns)\n",
        "\n",
        "# print the column names\n",
        "print(dummy_cols)"
      ],
      "metadata": {
        "id": "qqjeD0fVv1gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a crosstab between delay_15 and a specific dummy variable\n",
        "crosstab = pd.crosstab(train_df_with_dummies['delay_15'], df_binary['SIGLADES_Sydney'])\n",
        "\n",
        "# calculate the percentage of delays for each category of the dummy variable\n",
        "prob_delay = crosstab.loc[1] / crosstab.sum()\n",
        "\n",
        "# extract the probability when the dummy variable is 1\n",
        "prob_delay_1 = prob_delay.loc[1]\n",
        "\n",
        "print(prob_delay_1)"
      ],
      "metadata": {
        "id": "CDlDqldht3S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['OPERA_Aeromexico', 'OPERA_Air Canada', 'OPERA_Air France', 'OPERA_Alitalia', 'OPERA_American Airlines', 'OPERA_Austral', 'OPERA_Avianca', 'OPERA_British Airways', 'OPERA_Copa Air', 'OPERA_Delta Air', 'OPERA_Gol Trans', 'OPERA_Grupo LATAM', 'OPERA_Iberia', 'OPERA_JetSmart SPA', 'OPERA_K.L.M.', 'OPERA_Lacsa', 'OPERA_Latin American Wings', 'OPERA_Oceanair Linhas Aereas', 'OPERA_Plus Ultra Lineas Aereas', 'OPERA_Qantas Airways', 'OPERA_Sky Airline', 'OPERA_United Airlines', 'SIGLADES_Arica', 'SIGLADES_Asuncion', 'SIGLADES_Atlanta', 'SIGLADES_Auckland N.Z.', 'SIGLADES_Balmaceda', 'SIGLADES_Bariloche', 'SIGLADES_Bogota', 'SIGLADES_Buenos Aires', 'SIGLADES_Calama', 'SIGLADES_Cancun', 'SIGLADES_Castro (Chiloe)', 'SIGLADES_Cataratas Iguacu', 'SIGLADES_Ciudad de Mexico', 'SIGLADES_Ciudad de Panama', 'SIGLADES_Cochabamba', 'SIGLADES_Concepcion', 'SIGLADES_Copiapo', 'SIGLADES_Cordoba', 'SIGLADES_Curitiba, Bra.', 'SIGLADES_Dallas', 'SIGLADES_Florianapolis', 'SIGLADES_Guayaquil', 'SIGLADES_Houston', 'SIGLADES_Iquique', 'SIGLADES_Isla de Pascua', 'SIGLADES_La Paz', 'SIGLADES_La Serena', 'SIGLADES_Lima', 'SIGLADES_Londres', 'SIGLADES_Los Angeles', 'SIGLADES_Madrid', 'SIGLADES_Melbourne', 'SIGLADES_Mendoza', 'SIGLADES_Miami', 'SIGLADES_Montevideo', 'SIGLADES_Neuquen', 'SIGLADES_Nueva York', 'SIGLADES_Orlando', 'SIGLADES_Osorno', 'SIGLADES_Paris', 'SIGLADES_Pisco, Peru', 'SIGLADES_Puerto Montt', 'SIGLADES_Puerto Natales', 'SIGLADES_Puerto Stanley', 'SIGLADES_Punta Arenas', 'SIGLADES_Punta Cana', 'SIGLADES_Punta del Este', 'SIGLADES_Quito', 'SIGLADES_Rio de Janeiro', 'SIGLADES_Roma', 'SIGLADES_Rosario', 'SIGLADES_San Juan, Arg.', 'SIGLADES_Santa Cruz', 'SIGLADES_Sao Paulo', 'SIGLADES_Sydney', 'SIGLADES_Temuco', 'SIGLADES_Toronto', 'SIGLADES_Tucuman', 'SIGLADES_Ushuia', 'SIGLADES_Valdivia', 'SIGLADES_Washington']\n",
        "result_dict = {}\n",
        "\n",
        "for col in columns:\n",
        "    # create a crosstab between delay_15 and the current column\n",
        "    crosstab = pd.crosstab(train_df_with_dummies['delay_15'], df_binary[col])\n",
        "    # calculate the percentage of delays for each category of the current column\n",
        "    prob_delay = crosstab.loc[1] / crosstab.sum()\n",
        "    # extract the probability when the current column is 1\n",
        "    prob_delay_1 = prob_delay.iloc[1] if len(prob_delay) > 1 else 0\n",
        "    # extract the probability when the current column is 0\n",
        "    prob_delay_0 = prob_delay.iloc[0] if len(prob_delay) > 1 else 0\n",
        "    # add the current column and its corresponding probabilities to the result dictionary\n",
        "    result_dict[col] = {'prob_delay_1': prob_delay_1, 'prob_delay_0': prob_delay_0}\n",
        "\n",
        "print(result_dict)"
      ],
      "metadata": {
        "id": "rlSC20BHy7X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dictionary\n",
        "prob_dict = result_dict\n",
        "\n",
        "# define a function to match the column values with the keys in the dictionary and add the corresponding probabilities\n",
        "def add_probabilities(row):\n",
        "    opera = f\"OPERA_{row['OPERA']}\"\n",
        "    siglades = f\"SIGLADES_{row['SIGLADES']}\"\n",
        "    if opera in prob_dict:\n",
        "        row['prob_delay_1_opera'] = prob_dict[opera]['prob_delay_1']\n",
        "        row['prob_delay_0_opera'] = prob_dict[opera]['prob_delay_0']\n",
        "    else:\n",
        "        row['prob_delay_1_opera'] = 0\n",
        "        row['prob_delay_0_opera'] = 0\n",
        "    if siglades in prob_dict:\n",
        "        row['prob_delay_1_siglades'] = prob_dict[siglades]['prob_delay_1']\n",
        "        row['prob_delay_0_siglades'] = prob_dict[siglades]['prob_delay_0']\n",
        "    else:\n",
        "        row['prob_delay_1_siglades'] = 0\n",
        "        row['prob_delay_0_siglades'] = 0\n",
        "    return row\n",
        "\n",
        "# apply the function to each row of the dataframe\n",
        "train_df = train_df.apply(lambda row: add_probabilities(row), axis=1)\n"
      ],
      "metadata": {
        "id": "pQu4yWqT7B9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dictionary\n",
        "prob_dict = result_dict\n",
        "\n",
        "# define a function to match the column values with the keys in the dictionary and add the corresponding probabilities\n",
        "def add_probabilities(row):\n",
        "    opera = f\"OPERA_{row['OPERA']}\"\n",
        "    siglades = f\"SIGLADES_{row['SIGLADES']}\"\n",
        "    if opera in prob_dict:\n",
        "        row['prob_delay_1_opera'] = prob_dict[opera]['prob_delay_1']\n",
        "        row['prob_delay_0_opera'] = prob_dict[opera]['prob_delay_0']\n",
        "    else:\n",
        "        row['prob_delay_1_opera'] = 0\n",
        "        row['prob_delay_0_opera'] = 0\n",
        "    if siglades in prob_dict:\n",
        "        row['prob_delay_1_siglades'] = prob_dict[siglades]['prob_delay_1']\n",
        "        row['prob_delay_0_siglades'] = prob_dict[siglades]['prob_delay_0']\n",
        "    else:\n",
        "        row['prob_delay_1_siglades'] = 0\n",
        "        row['prob_delay_0_siglades'] = 0\n",
        "    return row\n",
        "\n",
        "# apply the function to each row of the dataframe\n",
        "test_df = test_df.apply(lambda row: add_probabilities(row), axis=1)"
      ],
      "metadata": {
        "id": "85bx1hRn-hEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph analisis and ratios"
      ],
      "metadata": {
        "id": "SS1AEmFMBA2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio between High season and delay\n",
        "count = df[(df['delay_15'] == 1) & (df['high_season'] == 1)].shape[0]\n",
        "count_2 = df[(df['high_season'] == 1)].shape[0]\n",
        "ratio = count / count_2\n",
        "count_3 = df[(df['delay_15'] == 1) & (df['high_season'] == 0)].shape[0]\n",
        "count_4 = df[(df['high_season'] == 0)].shape[0]\n",
        "ratio_2 = count_3 / count_4\n",
        "\n",
        "print ('Ratio referring to high season and delay:' )\n",
        "print (ratio)\n",
        "\n",
        "print ('Ratio referring to low season and delay:' )\n",
        "print (ratio_2)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lpssmuy98AG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio between High season and delay\n",
        "count = df[(df['delay_15'] == 1) & (df['period_day'] == 'night')].shape[0]\n",
        "count_2 = df[(df['period_day'] == 'night')].shape[0]\n",
        "ratio = count / count_2\n",
        "\n",
        "count_3 = df[(df['delay_15'] == 1) & (df['period_day'] == 'afternoon')].shape[0]\n",
        "count_4 = df[(df['period_day'] == 'afternoon')].shape[0]\n",
        "ratio_2 = count_3 / count_4\n",
        "\n",
        "count_5 = df[(df['delay_15'] == 1) & (df['period_day'] == 'morning')].shape[0]\n",
        "count_6 = df[(df['period_day'] == 'morning')].shape[0]\n",
        "ratio_3 = count_5 / count_6\n",
        "\n",
        "print ('Ratio referring to period day night delay:' )\n",
        "print (ratio)\n",
        "\n",
        "print ('Ratio referring to period day afternoon delay:' )\n",
        "print (ratio_2)\n",
        "\n",
        "print ('Ratio referring to period day morning delay:' )\n",
        "print (ratio_3)"
      ],
      "metadata": {
        "id": "yYuuRL3cBP2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period_day = df.groupby('delay_15')['period_day'].value_counts()\n",
        "TIPOVUELO = df.groupby('delay_15')['TIPOVUELO'].value_counts()\n",
        "SIGLAORI =  df.groupby('delay_15')['SIGLAORI'].value_counts()\n",
        "SIGLAORI =  df.groupby('delay_15')['SIGLAORI'].value_counts()\n",
        "SIGLADES =  df.groupby('delay_15')['SIGLADES'].value_counts()\n",
        "SIGLADES =  df.groupby('delay_15')['SIGLADES'].value_counts()\n",
        "OPERA =  df.groupby('delay_15')['OPERA'].value_counts()\n",
        "high_season =  df.groupby('delay_15')['high_season'].value_counts()\n",
        "MES = df.groupby('delay_15')['MES'].value_counts()\n",
        "DIA = df.groupby('delay_15')['DIA'].value_counts()\n",
        "AÑO = df.groupby('delay_15')['AÑO'].value_counts()\n",
        "DIANOM = df.groupby('delay_15')['DIANOM'].value_counts()"
      ],
      "metadata": {
        "id": "wDh1J_1mCDqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='SIGLADES', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('SIGLADES')\n",
        "plt.title('Counts of SIGLADES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JArIQTm5MRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are specific cities that have an influence on the delay."
      ],
      "metadata": {
        "id": "oTAeeeyGN3rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='period_day', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('period_day')\n",
        "plt.title('Counts of period_day by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95CeySlAN_ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delays usually occur in the afternoon or nights with the follow ratio Ratio Referring to period day night delay:\n",
        "0.20002305741295826\n",
        "Ratio referring to period day afternoon delay:\n",
        "0.19940406178938289\n",
        "Ratio referring to period day morning delay:\n",
        "0.16007258096327562"
      ],
      "metadata": {
        "id": "mOq7abynOLJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='MES', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('MES')\n",
        "plt.title('Counts of MES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hVxgRU5O4Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the month, it does provide information regarding the delay and it can be assimilated as that in holidays dates (December or July) there are considerable delays."
      ],
      "metadata": {
        "id": "dMJrOvdZRRCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='DIA', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('DIA')\n",
        "plt.title('Counts of MES by delay_15')\n",
        "plt.legend(labels=[])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLAXaPCnPLt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the day, it does not provide much information and there is an uniform distribution."
      ],
      "metadata": {
        "id": "piBH1lLMQwra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='DIANOM', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('DIANOM')\n",
        "plt.title('Counts of DIANOM by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dnUMIq9G4D6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting the afluent on saturday is lower than the rest of the days and in sunday looks like there is better chances to have a delay."
      ],
      "metadata": {
        "id": "zAak0ao44sCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='high_season', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('delay_15')\n",
        "plt.ylabel('high_season')\n",
        "plt.title('Counts of high_season by delay_15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htNWSPDa-4JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratio referring to high season (1) and delay:\n",
        "0.24372384937238495\n",
        "Ratio referring to low season (0) and delay:\n",
        "0.17422732236648233"
      ],
      "metadata": {
        "id": "W_eefIPv_f22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='OPERA', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('Flight Delayed by 15 Minutes or More')\n",
        "plt.ylabel('Airline Name')\n",
        "plt.title('Counts of Airline Names by Flight Delay')\n",
        "\n",
        "# adjust the x-axis\n",
        "plt.xticks(range(len(df['delay_15'].unique())), df['delay_15'].unique())\n",
        "plt.legend(labels=[])\n",
        "\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "z0ANa_H85Cgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much information on other flights mostly representing latam and sky airline."
      ],
      "metadata": {
        "id": "qjjKHmht_wKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='delay_15', hue='TIPOVUELO', data=df)\n",
        "\n",
        "# set axis labels and title\n",
        "plt.xlabel('Flight Delayed by 15 Minutes or More')\n",
        "plt.ylabel('TIPOVUELO')\n",
        "plt.title('Counts of TIPOVUELO by Flight Delay')\n",
        "\n",
        "# adjust the x-axis\n",
        "plt.xticks(range(len(df['delay_15'].unique())), df['delay_15'].unique())\n",
        "\n",
        "\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "FOG5Ze69AHgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "International flights are more prone to delays. But nothing conclusive."
      ],
      "metadata": {
        "id": "lbR_0925AlTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyoff\n",
        "import seaborn as sns\n",
        "\n",
        "corrmat = df.corr()\n",
        "fig = plt.figure(figsize = (12, 9))\n",
        "sns.heatmap(corrmat, vmax = .8, square = True, annot = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "onMUKOYnK0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.- Predictive model"
      ],
      "metadata": {
        "id": "bYEoBPRUBRhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data"
      ],
      "metadata": {
        "id": "TRoMyhjccqOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df.info(verbose=True)"
      ],
      "metadata": {
        "id": "4ZWbLyB8THM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "qfgyED_1pudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select columns 10,11,12,18,20,22,24,25,26,27,28,29\n",
        "cols_1 = [10,11,12,18,20,22,24,25,26,27,29,32,33,34,35,36]\n",
        "\n",
        "# Select columns 30 to 131\n",
        "#cols_2 = np.r_[30:132]\n",
        "\n",
        "# Combine both lists of columns\n",
        "selected_cols = cols_1 #+ list(cols_2)\n",
        "\n",
        "# Select the columns from the dataframe using iloc\n",
        "df_m_train = train_df.iloc[:, selected_cols]\n",
        "# Select the columns from the dataframe using iloc\n",
        "df_m_test = test_df.iloc[:, selected_cols]"
      ],
      "metadata": {
        "id": "wDd-Sk4dVOwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize data"
      ],
      "metadata": {
        "id": "X318x5-xB7qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# select numerical columns using iloc and dtypes\n",
        "numerical_cols = df_m_train.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# create MinMaxScaler instance and fit on numerical columns\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_m_train[numerical_cols])\n",
        "\n",
        "# apply transform and add new scaled columns with '_sc' suffix\n",
        "df_m_train = df_m_train.copy()  # create a copy of the original DataFrame\n",
        "df_m_train.loc[:, numerical_cols] = scaler.transform(df_m_train[numerical_cols])  # use loc for boolean indexing\n",
        "\n",
        "# add suffix to column names\n",
        "new_col_names = [col + '_sc' for col in numerical_cols]\n",
        "df_m_train.columns = list(df_m_train.columns[:-len(numerical_cols)]) + new_col_names"
      ],
      "metadata": {
        "id": "IudD4PllzgoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# select numerical columns using iloc and dtypes\n",
        "numerical_cols = df_m_test.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# create MinMaxScaler instance and fit on numerical columns\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_m_test[numerical_cols])\n",
        "\n",
        "# apply transform and add new scaled columns with '_sc' suffix\n",
        "df_m_test = df_m_test.copy()  # create a copy of the original DataFrame\n",
        "df_m_test.loc[:, numerical_cols] = scaler.transform(df_m_test[numerical_cols])  # use loc for boolean indexing\n",
        "\n",
        "# add suffix to column names\n",
        "new_col_names = [col + '_sc' for col in numerical_cols]\n",
        "df_m_test.columns = list(df_m_test.columns[:-len(numerical_cols)]) + new_col_names\n"
      ],
      "metadata": {
        "id": "_o4ImqNic08j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Separate the data into X (input features) and y (target variable)\n",
        "\n",
        "X_cols = list(set(df_m_train.columns)-set(['min_diff_sc','delay_15_sc','delay_15_rate_sc']))\n",
        "y_col = ['min_diff_sc']\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "CZ8Y66f1R6EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differents models"
      ],
      "metadata": {
        "id": "FAWUwaJScjcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n"
      ],
      "metadata": {
        "id": "2DiK46nPf_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = LinearRegression(fit_intercept=False, copy_X=False, n_jobs=-1, positive=True)\n",
        "\n",
        "# Create the parameter grid\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "# Perform grid search cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "# Use the best model to make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "ln_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "ln_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "ln_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "ln_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "print('Best params:', grid_search.best_params_)\n",
        "print('Best score:', -grid_search.best_score_)"
      ],
      "metadata": {
        "id": "9aajvP5vgJwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Define the decision tree regressor model\n",
        "dt = DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=None, min_samples_split=2, \n",
        "                           min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
        "                           max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "dt_best = grid_search.best_estimator_\n",
        "\n",
        "# Use the best model to make predictions\n",
        "y_pred = dt_best.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "dt_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "dt_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "dt_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "dt_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "print('Best params:', grid_search.best_params_)\n",
        "print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Root Mean Squared Error for Linear Regression: 0.06390763348078912\n",
        "#Variance score: 0.55\n",
        "#Mape: 544094622945.30\n",
        "#max_error: 0.74\n",
        "#Best params: {'criterion': 'friedman_mse', 'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 6, 'splitter': 'random'}\n",
        "#Best score: -0.5433921721352531"
      ],
      "metadata": {
        "id": "k0Pc8WlXiIUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.neighbors import KNeighborsRegressor\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train = df_m_train[X_cols].values\n",
        "#y_train = df_m_train[y_col].values\n",
        "#X_test = df_m_test[X_cols].values\n",
        "#y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model\n",
        "#model = KNeighborsRegressor()\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'n_neighbors': [100, 200, 300],\n",
        "#    'weights': ['uniform', 'distance'],\n",
        "#    'metric': ['euclidean', 'manhattan', 'chebyshev', 'canberra'],\n",
        "#    'leaf_size': [20, 30, 40]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Use the best hyperparameters to create the model\n",
        "#best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "#y_pred = best_model.predict(X_test)\n",
        "#print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "#kn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "#print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "#kn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "#print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "#kn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "#print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "#kn_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Root Mean Squared Error for Linear Regression: 0.063890948041988\n",
        "#Variance score: 0.55\n",
        "#Mape: 620469750833.88\n",
        "#max_error: 0.71\n",
        "#Best params: {'leaf_size': 40, 'metric': 'manhattan', 'n_neighbors': 100, 'weights': 'uniform'}\n",
        "#Best score: -0.5381708945834038"
      ],
      "metadata": {
        "id": "py5XzieJRAdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model with the best hyperparameters\n",
        "best_model = KNeighborsRegressor(n_neighbors=100, weights='uniform', metric='manhattan', leaf_size=40)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "y_pred = best_model.predict(X_test)\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "kn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "kn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "kn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "kn_e4= (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mLatP9iHCD9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import xgboost as xgb\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, max_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train = df_m_train[X_cols].values\n",
        "#y_train = df_m_train[y_col].values\n",
        "#X_test = df_m_test[X_cols].values\n",
        "#y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "#xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=40, max_depth= 500, \n",
        "#                             eta = 0.2, min_child_weight=1, alpha= 5, num_parallel_tree= 5)\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'max_depth': [3, 5, 7, 10],\n",
        "#    'min_child_weight': [1, 3, 5],\n",
        "#    'subsample': [0.6, 0.8, 1.0],\n",
        "#    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "#    'learning_rate': [0.1, 0.01, 0.001]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "#best_xgb_model = grid_search.best_estimator_\n",
        "#y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Compute the evaluation metrics for the best model\n",
        "#print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "#xgb_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "#print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "#xgb_e2= (metrics.r2_score(y_test, y_pred))\n",
        "#print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "#xgb_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "#print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "#xgb_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0}\n",
        "#Root Mean Squared Error for Linear Regression: 0.0624065238448978\n",
        "#Variance score: 0.57\n",
        "#Mape: 562123968452.08\n",
        "#max_error: 0.73\n",
        "#Best params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0}\n",
        "#Best score: -0.5531640844928655"
      ],
      "metadata": {
        "id": "gx0bGcLwmc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, max_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=40, max_depth=7, min_child_weight=1, \n",
        "                                  subsample=0.8, colsample_bytree=0.6, learning_rate=0.1, num_parallel_tree=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Compute the evaluation metrics for the best model\n",
        "print('Root Mean Squared Error for XGBoost:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "xgb_e1 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (r2_score(y_test, y_pred)))\n",
        "xgb_e2 = r2_score(y_test, y_pred)\n",
        "print('Mape: %.2f' % (mean_absolute_percentage_error(y_test, y_pred)))\n",
        "xgb_e3 = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print('max_error: %.2f' % (max_error(y_test, y_pred)))\n",
        "xgb_e4 = max_error(y_test, y_pred)"
      ],
      "metadata": {
        "id": "AptOA0FlDKXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the feature importances\n",
        "xgb.plot_importance(best_xgb_model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2ZMmLC3jyp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cols"
      ],
      "metadata": {
        "id": "MfyrpQ2WDnjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "nn_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "nn_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "nn_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "nn_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "og_YK9fFWtZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define a function to create the model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "# Create the model using the best hyperparameters\n",
        "model = create_model()\n",
        "\n",
        "# Train the model on the full training set\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "nn2_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "nn2_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "nn2_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "nn2_e4= (metrics.max_error(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "mfF22uLolVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code used to find the params for nn model."
      ],
      "metadata": {
        "id": "oMzal989-7Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to create the model\n",
        "#def create_model(activation='relu', neurons=64, dropout_rate=0.5):\n",
        "#    model = Sequential()\n",
        "#    model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
        "#    model.add(Dropout(dropout_rate))\n",
        "#    model.add(Dense(neurons//2, activation=activation))\n",
        "#    model.add(Dropout(dropout_rate))\n",
        "#    model.add(Dense(1))\n",
        "#    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "#    return model\n",
        "\n",
        "# Create a KerasRegressor object for use in GridSearchCV\n",
        "#model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "#param_grid = {\n",
        "#    'activation': ['relu', 'tanh', 'sigmoid'],\n",
        "#    'neurons': [32, 64, 128],\n",
        "#    'dropout_rate': [0.2, 0.5, 0.8]\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)"
      ],
      "metadata": {
        "id": "tYDDdhquXUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code used to find the params for svr model"
      ],
      "metadata": {
        "id": "oWrlChJE_Bkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.svm import SVR\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#X_train = df_m_train[X_cols].values\n",
        "#y_train = df_m_train[y_col].values\n",
        "#X_test = df_m_test[X_cols].values\n",
        "#y_test = df_m_test[y_col].values\n",
        "\n",
        "# Define the parameter grid\n",
        "#param_grid = {\n",
        "#    'C': [0.1, 1, 10, 100],\n",
        "#    'gamma': [0.01, 0.1, 1, 'scale'],\n",
        "#    'kernel': ['linear', 'rbf', 'poly']\n",
        "#}\n",
        "\n",
        "# Create a GridSearchCV object and fit to the training data\n",
        "#svr = SVR()\n",
        "#grid_search = GridSearchCV(svr, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding mean test score\n",
        "#print('Best params:', grid_search.best_params_)\n",
        "#print('Best score:', -grid_search.best_score_)\n",
        "\n",
        "#Best params: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
        "#Best score: -0.5193479380405777"
      ],
      "metadata": {
        "id": "d_A1vaZ68uBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Train the SVR model with the best hyperparameters\n",
        "svr = SVR(kernel='rbf', C=10, gamma=0.01)\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error for Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "svr_e1= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "svr_e2= (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "svr_e3= (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "svr_e4= (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "pySz09NzkgkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stack model"
      ],
      "metadata": {
        "id": "4kE1mxUsVI8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we going to analize differents metrics of every model."
      ],
      "metadata": {
        "id": "uHjkfPM53nJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the error arrays\n",
        "dt_error = [dt_e1, dt_e2, dt_e3, dt_e4]\n",
        "xgb_error = [xgb_e1, xgb_e2, xgb_e3, xgb_e4]\n",
        "nn_error = [nn_e1, nn_e2, nn_e3, nn_e4]\n",
        "ln_error = [ln_e1, ln_e2, ln_e3, ln_e4]\n",
        "nn2_error = [nn2_e1, nn2_e2, nn2_e3, nn2_e4]\n",
        "svr_error = [svr_e1, svr_e2, svr_e3, svr_e4]\n",
        "\n",
        "# Calculate the mean of each error metric\n",
        "rmse = [np.mean(dt_error), np.mean(xgb_error), np.mean(nn_error), np.mean(ln_error), np.mean(nn2_error), np.mean(svr_error)]\n",
        "variance_score = [np.mean(dt_e2), np.mean(xgb_e2), np.mean(nn_e2), np.mean(ln_e2), np.mean(nn2_e2), np.mean(svr_e2)]\n",
        "mape = [np.mean(dt_e3), np.mean(xgb_e3), np.mean(nn_e3), np.mean(ln_e3), np.mean(nn2_e3), np.mean(svr_e3)]\n",
        "max_error = [np.mean(dt_e4), np.mean(xgb_e4), np.mean(nn_e4), np.mean(ln_e4), np.mean(nn2_e4), np.mean(svr_e4)]\n",
        "\n",
        "# Plot the error metrics for each model\n",
        "labels = ['Decision Tree', 'XGBoost', 'Neural Network', 'Linear Regression', 'Neural Network 2', 'SVR']\n",
        "\n",
        "# RMSE and MAPE plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, rmse, width, label='RMSE')\n",
        "plt.bar(x + width/2, mape, width, label='MAPE')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('RMSE and MAPE Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Variance score and Max Error plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, variance_score, width, label='Variance score')\n",
        "plt.bar(x + width/2, max_error, width, label='Max Error')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('Variance score and Max Error Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the mean of each metric\n",
        "print('Mean RMSE:', np.mean(rmse))\n",
        "print('Mean Variance Score:', np.mean(variance_score))\n",
        "print('Mean MAPE:', np.mean(mape))\n",
        "print('Mean Max Error:', np.mean(max_error))"
      ],
      "metadata": {
        "id": "osaFvI4Q20oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "# Initialize the base models\n",
        "estimators = [('xgb', XGBRegressor(colsample_bytree=1.0, learning_rate=0.1, max_depth=10, min_child_weight=1, subsample=1.0)),\n",
        "              ('dt', DecisionTreeRegressor(criterion='friedman_mse', max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=6, splitter='random'))]\n",
        "\n",
        "# Initialize the stacking model\n",
        "stack_model = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor())\n",
        "\n",
        "# Train the stacking model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "stack_e1 = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "stack_e2 = (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "stack_e3 = (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "stack_e4 = (metrics.max_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "_i0VKJJ6MGsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to find the best params via gridsearch and using xgb model to ensamble.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qF9iTBjP_ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train = df_m_train[X_cols].values\n",
        "y_train = df_m_train[y_col].values\n",
        "X_test = df_m_test[X_cols].values\n",
        "y_test = df_m_test[y_col].values\n",
        "\n",
        "\n",
        "# Initialize the base models\n",
        "estimators = [('xgb', XGBRegressor(colsample_bytree=1.0, learning_rate=0.1, max_depth=10, min_child_weight=1, subsample=1.0)),\n",
        "              ('dt', DecisionTreeRegressor(criterion='friedman_mse', max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=6, splitter='random'))]\n",
        "\n",
        "# Set the parameter grid for XGBoost\n",
        "xgb_params = {'xgb__learning_rate': [0.01, 0.1, 0.5],\n",
        "              'xgb__max_depth': [3, 5, 10],\n",
        "              'xgb__subsample': [0.5, 0.8, 1.0],\n",
        "              'xgb__colsample_bytree': [0.5, 0.8, 1.0]}\n",
        "\n",
        "# Initialize the stacking model\n",
        "stack_model = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor())\n",
        "\n",
        "# Initialize the grid search object\n",
        "grid_search = GridSearchCV(stack_model, param_grid={'xgb__learning_rate': [0.01, 0.1, 0.5],\n",
        "                                                    'xgb__max_depth': [3, 5, 10],\n",
        "                                                    'xgb__subsample': [0.5, 0.8, 1.0],\n",
        "                                                    'xgb__colsample_bytree': [0.5, 0.8, 1.0]}, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Train the stacking model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "stack_e1 = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Variance score: %.2f' % (metrics.r2_score(y_test, y_pred)))\n",
        "stack_e2 = (metrics.r2_score(y_test, y_pred))\n",
        "print('Mape: %.2f' % (metrics.mean_absolute_percentage_error(y_test, y_pred)))\n",
        "stack_e3 = (metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
        "print('max_error: %.2f' % (metrics.max_error(y_test, y_pred)))\n",
        "stack_e4 = (metrics.max_error(y_test, y_pred)) \n",
        "\n"
      ],
      "metadata": {
        "id": "CNS8Av0iMSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the error arrays\n",
        "dt_error = [dt_e1, dt_e2, dt_e3, dt_e4]\n",
        "xgb_error = [xgb_e1, xgb_e2, xgb_e3, xgb_e4]\n",
        "stack_error = [stack_e1, stack_e2, stack_e3, stack_e4]\n",
        "\n",
        "# Calculate the mean of each error metric\n",
        "rmse = [np.mean(dt_error), np.mean(xgb_error), np.mean(stack_error)]\n",
        "variance_score = [np.mean(dt_e2), np.mean(xgb_e2), np.mean(stack_e2)]\n",
        "mape = [np.mean(dt_e3), np.mean(xgb_e3), np.mean(stack_e3)]\n",
        "max_error = [np.mean(dt_e4), np.mean(xgb_e4),  np.mean(stack_e4)]\n",
        "\n",
        "# Plot the error metrics for each model\n",
        "labels = ['Decision Tree', 'XGBoost', 'Stacking']\n",
        "\n",
        "# RMSE and MAPE plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, rmse, width, label='RMSE')\n",
        "plt.bar(x + width/2, mape, width, label='MAPE')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('RMSE and MAPE Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# variance score and Max Error plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, variance_score, width, label='Variance score')\n",
        "plt.bar(x + width/2, max_error, width, label='Max Error')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Error Metric')\n",
        "plt.title('Variance score and Max Error Comparison')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the mean of each metric\n",
        "print('Mean RMSE:', np.mean(rmse))\n",
        "print('Mean Variance Score:', np.mean(variance_score))\n",
        "print('Mean MAPE:', np.mean(mape))\n",
        "print('Mean Max Error:', np.mean(max_error))\n",
        "\n"
      ],
      "metadata": {
        "id": "MCbLghdIN0mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.- Results"
      ],
      "metadata": {
        "id": "5EqkSZ2-miHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.- Comments"
      ],
      "metadata": {
        "id": "zk9qftXOmq4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "recommendations to improve the model for predicting delays in airplane takeoff and landing:\n",
        "\n",
        "Add weather information: Weather can have a significant impact on flight delays, so incorporating weather data into the model can help improve its accuracy. Factors such as temperature, precipitation, wind speed, and cloud cover can all affect flight operations.\n",
        "\n",
        "Include additional variables: Adding variables such as gate location, flight distance, online boarding, baggage handling, inflight wifi service, satisfaction, customer type, and gender can provide more information about the factors that contribute to flight delays. This can help the model identify patterns and relationships that may not be captured by the existing variables.\n",
        "\n",
        "Manage outliers: Outliers can have a significant impact on model performance, so it is important to identify and manage them appropriately. This can involve removing outliers or transforming the data to reduce their impact.\n",
        "\n",
        "Use more data: Using more data beyond just the 2017 dataset can provide a more comprehensive understanding of the problem and help the model identify more patterns and relationships. This can lead to better accuracy and generalization of the model.\n",
        "\n",
        "Optimize execution time: As the size of the dataset and complexity of the model increases, it may become necessary to optimize the execution time of the model. This can involve techniques such as parallel processing, distributed computing, or using more efficient algorithms.\n",
        "\n",
        "Overall, these recommendations can help improve the accuracy and generalization of the model for predicting delays in airplane takeoff and landing. By incorporating more variables, managing outliers, using more data, and optimizing execution time, the model can better capture the factors that contribute to flight delays and provide more accurate predictions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "97SEO4LN5PHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper \"Flight delay prediction based on deep learning and Levenberg-Marquart algorithm\", the Levenberg-Marquardt algorithm is used as an optimization algorithm for training the neural network. Specifically, the algorithm is used to minimize the mean squared error between the predicted and actual flight delay values.\n",
        "\n",
        "The Levenberg-Marquardt algorithm is a commonly used optimization algorithm for training neural networks. It combines the steepest descent method with a trust region approach to achieve fast convergence to a local minimum of the objective function. It is particularly well-suited for solving nonlinear least squares problems, which makes it a popular choice for training neural networks."
      ],
      "metadata": {
        "id": "PspfpXpyG-3Y"
      }
    }
  ]
}